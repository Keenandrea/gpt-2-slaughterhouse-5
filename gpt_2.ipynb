{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keenandrea/gpt-2-slaughterhouse-5/blob/master/gpt_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cz0DHJbTOuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/nshepperd/gpt-2.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2fhjjg8T-jJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23fccb2a-715b-40e4-d466-a0923daba080"
      },
      "source": [
        "cd gpt-2"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUnMGnBBUCyP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "outputId": "6cdac07c-7edd-4bb2-fdd6-e33e770ee4a1"
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/b7/205702f348aab198baecd1d8344a90748cb68f53bdcd1cc30cbc08e47d3e/fire-0.1.3.tar.gz\n",
            "Collecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 21.3MB/s \n",
            "\u001b[?25hCollecting toposort==1.5 (from -r requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/1a/4d/6b30377c3051e76559d1185c1dbbfff15aed31f87acdd14c22\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "\u001b[31mERROR: spacy 2.0.18 has requirement regex==2018.01.10, but you'll have regex 2017.4.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, regex, tqdm, toposort\n",
            "  Found existing installation: regex 2018.1.10\n",
            "    Uninstalling regex-2018.1.10:\n",
            "      Successfully uninstalled regex-2018.1.10\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.1.3 regex-2017.4.5 toposort-1.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryzKALueU1JV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "5fb3c355-439c-4a06-ec76-9f9f8f1dd763"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG06pzsuVAc7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "outputId": "fa0af921-8705-44c5-a6f0-e7569aed68dc"
      },
      "source": [
        "!python3 download_model.py 345M"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 926kit/s]                                                      \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 55.6Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.08Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:20, 70.1Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 8.24Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 48.2Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 45.3Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAeklCOKVkR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbdRc-UwV5UC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/drive/My\\ Drive/checkpoint/ /content/gpt-2/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c49Yv7cEWHkp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "036fb760-a546-4a77-d082-70c8e1b60616"
      },
      "source": [
        "!pip install toposort"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: toposort in /usr/local/lib/python3.6/dist-packages (1.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz83OG--Wixo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 30331
        },
        "outputId": "2c8104b2-7680-4305-f919-a83e7197c909"
      },
      "source": [
        "!PYTHONPATH=src ./train.py --dataset /content/gpt-2/slaughterhouse_five.txt --model_name '345M'"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-25 05:25:59.794931: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-25 05:25:59.795227: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x15a8680 executing computations on platform Host. Devices:\n",
            "2019-05-25 05:25:59.795262: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-25 05:25:59.994123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-25 05:25:59.994729: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x15a7fa0 executing computations on platform CUDA. Devices:\n",
            "2019-05-25 05:25:59.994764: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-25 05:25:59.995174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-25 05:25:59.995200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-25 05:26:00.554305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-25 05:26:00.554386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-25 05:26:00.554402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-25 05:26:00.554753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:00<00:00,  1.50it/s]\n",
            "dataset has 74534 tokens\n",
            "Training...\n",
            "2019-05-25 05:26:46.189646: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "[1 | 8.85] loss=3.40 avg=3.40\n",
            "[2 | 10.39] loss=3.08 avg=3.24\n",
            "[3 | 11.92] loss=3.30 avg=3.26\n",
            "[4 | 13.45] loss=2.84 avg=3.15\n",
            "[5 | 14.99] loss=2.93 avg=3.11\n",
            "[6 | 16.52] loss=2.86 avg=3.06\n",
            "[7 | 18.07] loss=3.07 avg=3.06\n",
            "[8 | 19.62] loss=2.89 avg=3.04\n",
            "[9 | 21.17] loss=3.14 avg=3.05\n",
            "[10 | 22.73] loss=2.91 avg=3.04\n",
            "[11 | 24.28] loss=3.05 avg=3.04\n",
            "[12 | 25.84] loss=3.09 avg=3.04\n",
            "[13 | 27.40] loss=3.03 avg=3.04\n",
            "[14 | 28.97] loss=2.96 avg=3.04\n",
            "[15 | 30.54] loss=2.87 avg=3.03\n",
            "[16 | 32.11] loss=2.80 avg=3.01\n",
            "[17 | 33.68] loss=2.86 avg=3.00\n",
            "[18 | 35.27] loss=3.03 avg=3.00\n",
            "[19 | 36.84] loss=2.96 avg=3.00\n",
            "[20 | 38.43] loss=2.77 avg=2.99\n",
            "[21 | 40.02] loss=2.97 avg=2.99\n",
            "[22 | 41.61] loss=2.92 avg=2.98\n",
            "[23 | 43.21] loss=2.99 avg=2.98\n",
            "[24 | 44.81] loss=2.95 avg=2.98\n",
            "[25 | 46.41] loss=2.59 avg=2.96\n",
            "[26 | 48.01] loss=3.04 avg=2.97\n",
            "[27 | 49.63] loss=2.89 avg=2.96\n",
            "[28 | 51.24] loss=3.14 avg=2.97\n",
            "[29 | 52.86] loss=2.98 avg=2.97\n",
            "[30 | 54.48] loss=2.76 avg=2.96\n",
            "[31 | 56.11] loss=2.86 avg=2.96\n",
            "[32 | 57.74] loss=2.62 avg=2.95\n",
            "[33 | 59.38] loss=2.86 avg=2.94\n",
            "[34 | 61.02] loss=2.88 avg=2.94\n",
            "[35 | 62.66] loss=2.72 avg=2.93\n",
            "[36 | 64.31] loss=2.87 avg=2.93\n",
            "[37 | 65.96] loss=2.98 avg=2.93\n",
            "[38 | 67.61] loss=2.97 avg=2.94\n",
            "[39 | 69.27] loss=2.67 avg=2.93\n",
            "[40 | 70.93] loss=2.95 avg=2.93\n",
            "[41 | 72.58] loss=2.82 avg=2.92\n",
            "[42 | 74.23] loss=2.93 avg=2.92\n",
            "[43 | 75.88] loss=2.38 avg=2.91\n",
            "[44 | 77.53] loss=3.02 avg=2.91\n",
            "[45 | 79.16] loss=2.85 avg=2.91\n",
            "[46 | 80.80] loss=2.77 avg=2.91\n",
            "[47 | 82.44] loss=3.01 avg=2.91\n",
            "[48 | 84.07] loss=2.79 avg=2.91\n",
            "[49 | 85.70] loss=2.70 avg=2.90\n",
            "[50 | 87.33] loss=2.86 avg=2.90\n",
            "[51 | 88.95] loss=2.85 avg=2.90\n",
            "[52 | 90.58] loss=2.86 avg=2.90\n",
            "[53 | 92.20] loss=2.85 avg=2.90\n",
            "[54 | 93.82] loss=2.63 avg=2.89\n",
            "[55 | 95.45] loss=2.73 avg=2.89\n",
            "[56 | 97.06] loss=2.62 avg=2.88\n",
            "[57 | 98.68] loss=2.75 avg=2.88\n",
            "[58 | 100.29] loss=2.51 avg=2.87\n",
            "[59 | 101.90] loss=2.54 avg=2.86\n",
            "[60 | 103.51] loss=2.56 avg=2.85\n",
            "[61 | 105.12] loss=2.83 avg=2.85\n",
            "[62 | 106.73] loss=2.63 avg=2.85\n",
            "[63 | 108.34] loss=2.88 avg=2.85\n",
            "[64 | 109.95] loss=2.47 avg=2.84\n",
            "[65 | 111.56] loss=2.54 avg=2.84\n",
            "[66 | 113.17] loss=2.41 avg=2.83\n",
            "[67 | 114.78] loss=2.98 avg=2.83\n",
            "[68 | 116.38] loss=2.51 avg=2.82\n",
            "[69 | 117.99] loss=2.82 avg=2.82\n",
            "[70 | 119.60] loss=2.62 avg=2.82\n",
            "[71 | 121.21] loss=2.94 avg=2.82\n",
            "[72 | 122.82] loss=2.64 avg=2.82\n",
            "[73 | 124.43] loss=2.75 avg=2.82\n",
            "[74 | 126.04] loss=2.74 avg=2.82\n",
            "[75 | 127.65] loss=2.57 avg=2.81\n",
            "[76 | 129.25] loss=2.49 avg=2.80\n",
            "[77 | 130.86] loss=2.63 avg=2.80\n",
            "[78 | 132.47] loss=2.67 avg=2.80\n",
            "[79 | 134.09] loss=2.52 avg=2.79\n",
            "[80 | 135.71] loss=2.45 avg=2.79\n",
            "[81 | 137.32] loss=2.79 avg=2.79\n",
            "[82 | 138.94] loss=2.60 avg=2.78\n",
            "[83 | 140.56] loss=2.60 avg=2.78\n",
            "[84 | 142.18] loss=2.91 avg=2.78\n",
            "[85 | 143.80] loss=2.38 avg=2.78\n",
            "[86 | 145.43] loss=2.22 avg=2.77\n",
            "[87 | 147.05] loss=2.56 avg=2.76\n",
            "[88 | 148.67] loss=2.62 avg=2.76\n",
            "[89 | 150.30] loss=2.31 avg=2.75\n",
            "[90 | 151.92] loss=2.64 avg=2.75\n",
            "[91 | 153.55] loss=2.45 avg=2.75\n",
            "[92 | 155.18] loss=2.87 avg=2.75\n",
            "[93 | 156.81] loss=2.51 avg=2.74\n",
            "[94 | 158.43] loss=2.98 avg=2.75\n",
            "[95 | 160.05] loss=2.39 avg=2.74\n",
            "[96 | 161.68] loss=2.49 avg=2.74\n",
            "[97 | 163.31] loss=2.58 avg=2.74\n",
            "[98 | 164.93] loss=2.17 avg=2.73\n",
            "[99 | 166.56] loss=2.72 avg=2.73\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " room, and he came out with, who knows why? He had been shot.\n",
            "\n",
            "But it never made sense, like, 'Well, you know, it didn't really happen.' You know, I didn't think that at all, to be honest with you, but you're gonna look something like that, and you're gonna think it happened, and you're gonna be like, 'Oh my God, is this real or is this made up?'\n",
            "\n",
            "And that's it. That's the only time. In my head, I didn't think of it and didn't really notice. And that was that. I wasn't the only one who was like that. The other two, of course, I just was. The others who were like that were the kids I was talking to in the theater, of course. They could see it. They weren't so blind. They didn't hide it from anyone. If somebody said they had seen a girl in a wheelchair, they were so dumbfounded they couldn't help but ask, 'Oh, is that all that there is?'\n",
            "\n",
            "Now in retrospect, do you miss that time? Do you know that it was when you were fourteen that you were in the hospital?\n",
            "\n",
            "I don't think so. I think I've sort of forgotten about that.\n",
            "\n",
            "Well, I always wanted to. I never imagined in my wildest dreams that I would be in a wheelchair.\n",
            "\n",
            "I had my own car when I was growing up, but I always wanted to drive it to school, too. And that's another one. I really loved driving the car to class. It was the most beautiful thing. I never would have been in a wheelchair in the car, even though I was a child, if I hadn't loved the car as much as I did.\n",
            "\n",
            "But of course, no one wanted me to play by myself. Some would go over for a drink. Well, there were other ladies who used to come over, even though they weren't allowed to. I wasn't allowed to see anyone but my friends and family—and I used to be really into girls a lot. I used to be the only one who didn't like boys a lot.\n",
            "\n",
            "So I had to sit in somebody else's lap, because everybody else was busy and I was alone. And the rest of the time I was busy with my friends and with my car's engines, and the kids were busy and I was working on my books and my papers.\n",
            "\n",
            "I used to sit in somebody else's lap all the time. People would come into the theater and ask me to play along. I had to make fun of them to them. And I loved being the center of attention. I used to do it on purpose, because those few people who went up and played with me would go and visit my parents and my sister and her husband. They came to see me just because I was their only friend, because I was the only one who wasn't a recluse. They'd see my head on the stage and think I was so cute and beautiful. That's what I do to myself. I'm the center of attention.\n",
            "\n",
            "Nobody ever questioned why I came to see the plays so often. They just didn't seem to care, really. Nobody told me that.\n",
            "\n",
            "You were always so happy, so happy to be there that you almost wished you hadn't been alive. It was so comforting to know there was somebody out there looking out for you.\n",
            "\n",
            "I couldn't imagine not being happy for anybody.\n",
            "\n",
            "Well, as much as we might not have liked each other all that much, there wasn't any animosity there. I think everybody else had a pretty good relationship with everybody else, and we didn't have to fight. We just weren't in that fight to the death.\n",
            "\n",
            "No one was out there complaining about anyone else. Nobody gave you any reason not to give, or to try to, or to not try.\n",
            "\n",
            "In those days there were no laws on the books against incest. It just never happened that way. There was nothing written into the laws on the books in those days. So if anybody was trying to have a child out there, he was up to his mother's neck in anything he might try to do. And he was always in charge, and he had to go first.\n",
            "\n",
            "I know a lot of parents who say now that had it not been for the war, they probably would have had two or three daughters. Now there aren't a lot. There's not much love for them in Washington, D.C.\n",
            "\n",
            "But all the time I was playing and talking to my friends and my brothers about girls and boys, and they still loved me and were still good to me, I was happy, because I had friends like that who could watch and care for me when I was too ill or too sick to play.\n",
            "\n",
            "That's another thing. I used to be the only\n",
            "\n",
            "[100 | 192.79] loss=2.84 avg=2.73\n",
            "[101 | 194.40] loss=2.57 avg=2.73\n",
            "[102 | 196.02] loss=2.67 avg=2.73\n",
            "[103 | 197.64] loss=2.40 avg=2.72\n",
            "[104 | 199.26] loss=2.54 avg=2.72\n",
            "[105 | 200.88] loss=2.54 avg=2.71\n",
            "[106 | 202.49] loss=2.73 avg=2.72\n",
            "[107 | 204.11] loss=2.53 avg=2.71\n",
            "[108 | 205.73] loss=2.92 avg=2.72\n",
            "[109 | 207.34] loss=2.87 avg=2.72\n",
            "[110 | 208.95] loss=2.58 avg=2.72\n",
            "[111 | 210.56] loss=2.87 avg=2.72\n",
            "[112 | 212.17] loss=3.11 avg=2.72\n",
            "[113 | 213.78] loss=2.40 avg=2.72\n",
            "[114 | 215.40] loss=2.37 avg=2.71\n",
            "[115 | 217.01] loss=2.70 avg=2.71\n",
            "[116 | 218.63] loss=2.83 avg=2.72\n",
            "[117 | 220.25] loss=2.72 avg=2.72\n",
            "[118 | 221.86] loss=2.51 avg=2.71\n",
            "[119 | 223.47] loss=2.81 avg=2.71\n",
            "[120 | 225.09] loss=3.23 avg=2.72\n",
            "[121 | 226.70] loss=3.00 avg=2.73\n",
            "[122 | 228.32] loss=2.59 avg=2.72\n",
            "[123 | 229.93] loss=2.13 avg=2.71\n",
            "[124 | 231.54] loss=2.63 avg=2.71\n",
            "[125 | 233.16] loss=2.64 avg=2.71\n",
            "[126 | 234.77] loss=2.52 avg=2.71\n",
            "[127 | 236.39] loss=2.66 avg=2.71\n",
            "[128 | 238.01] loss=2.37 avg=2.70\n",
            "[129 | 239.62] loss=2.73 avg=2.70\n",
            "[130 | 241.24] loss=2.72 avg=2.71\n",
            "[131 | 242.86] loss=2.77 avg=2.71\n",
            "[132 | 244.47] loss=3.05 avg=2.71\n",
            "[133 | 246.08] loss=2.25 avg=2.70\n",
            "[134 | 247.69] loss=2.13 avg=2.70\n",
            "[135 | 249.31] loss=2.72 avg=2.70\n",
            "[136 | 250.93] loss=2.48 avg=2.69\n",
            "[137 | 252.54] loss=2.35 avg=2.69\n",
            "[138 | 254.16] loss=2.58 avg=2.69\n",
            "[139 | 255.77] loss=2.49 avg=2.69\n",
            "[140 | 257.39] loss=2.22 avg=2.68\n",
            "[141 | 259.01] loss=2.35 avg=2.67\n",
            "[142 | 260.63] loss=2.25 avg=2.67\n",
            "[143 | 262.24] loss=2.39 avg=2.67\n",
            "[144 | 263.86] loss=2.29 avg=2.66\n",
            "[145 | 265.47] loss=2.50 avg=2.66\n",
            "[146 | 267.09] loss=2.27 avg=2.65\n",
            "[147 | 268.71] loss=2.55 avg=2.65\n",
            "[148 | 270.32] loss=2.55 avg=2.65\n",
            "[149 | 271.94] loss=2.28 avg=2.65\n",
            "[150 | 273.56] loss=2.71 avg=2.65\n",
            "[151 | 275.17] loss=2.43 avg=2.64\n",
            "[152 | 276.79] loss=2.82 avg=2.65\n",
            "[153 | 278.42] loss=2.82 avg=2.65\n",
            "[154 | 280.04] loss=2.05 avg=2.64\n",
            "[155 | 281.65] loss=2.38 avg=2.64\n",
            "[156 | 283.27] loss=2.41 avg=2.63\n",
            "[157 | 284.89] loss=2.56 avg=2.63\n",
            "[158 | 286.51] loss=2.42 avg=2.63\n",
            "[159 | 288.13] loss=2.11 avg=2.62\n",
            "[160 | 289.74] loss=2.16 avg=2.62\n",
            "[161 | 291.36] loss=2.48 avg=2.62\n",
            "[162 | 292.98] loss=2.50 avg=2.62\n",
            "[163 | 294.59] loss=2.04 avg=2.61\n",
            "[164 | 296.21] loss=2.45 avg=2.61\n",
            "[165 | 297.83] loss=2.20 avg=2.60\n",
            "[166 | 299.45] loss=2.70 avg=2.60\n",
            "[167 | 301.06] loss=2.71 avg=2.60\n",
            "[168 | 302.68] loss=2.63 avg=2.60\n",
            "[169 | 304.30] loss=2.52 avg=2.60\n",
            "[170 | 305.92] loss=2.83 avg=2.61\n",
            "[171 | 307.54] loss=1.77 avg=2.60\n",
            "[172 | 309.15] loss=2.49 avg=2.59\n",
            "[173 | 310.77] loss=1.84 avg=2.59\n",
            "[174 | 312.38] loss=2.40 avg=2.58\n",
            "[175 | 314.00] loss=2.19 avg=2.58\n",
            "[176 | 315.62] loss=2.69 avg=2.58\n",
            "[177 | 317.24] loss=1.99 avg=2.57\n",
            "[178 | 318.86] loss=2.16 avg=2.57\n",
            "[179 | 320.48] loss=2.36 avg=2.57\n",
            "[180 | 322.09] loss=2.07 avg=2.56\n",
            "[181 | 323.71] loss=2.27 avg=2.56\n",
            "[182 | 325.33] loss=2.46 avg=2.55\n",
            "[183 | 326.94] loss=1.84 avg=2.55\n",
            "[184 | 328.56] loss=2.23 avg=2.54\n",
            "[185 | 330.18] loss=2.20 avg=2.54\n",
            "[186 | 331.80] loss=2.17 avg=2.53\n",
            "[187 | 333.42] loss=2.38 avg=2.53\n",
            "[188 | 335.03] loss=2.36 avg=2.53\n",
            "[189 | 336.65] loss=2.99 avg=2.54\n",
            "[190 | 338.27] loss=2.27 avg=2.53\n",
            "[191 | 339.89] loss=2.21 avg=2.53\n",
            "[192 | 341.50] loss=2.64 avg=2.53\n",
            "[193 | 343.13] loss=2.34 avg=2.53\n",
            "[194 | 344.74] loss=2.19 avg=2.52\n",
            "[195 | 346.36] loss=1.90 avg=2.52\n",
            "[196 | 347.98] loss=2.87 avg=2.52\n",
            "[197 | 349.60] loss=2.27 avg=2.52\n",
            "[198 | 351.22] loss=2.18 avg=2.51\n",
            "[199 | 352.84] loss=2.07 avg=2.51\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " the one in my book, about a British pilot (who wasn't a pilot) killed in combat. It's like the whole goddamned thing is a movie made for TV.\n",
            "\n",
            "And this is actually the one that I'm looking for these days. If you can find it, let me know. I'm writing a novel about a man who's been in combat in Vietnam, and it's part memoir, part play, and part documentary — you get the picture. The man's been in Vietnam for 10, 12 years, hasn't he? So he knows the whole story. He doesn't invent it. And he writes a novel. But then he goes on TV. All his fellow soldiers read the goddamn book, too. So he's on TV again, right now. I wrote a novel about a British pilot who died in combat in Vietnam. He's the guy who gives the speech about the war after the raid, and he wrote it. So it goes.\n",
            "\n",
            "There is a second American in the book who has just been killed in the battle. When he dies, his body doesn't go back to England. It goes to the National War Memorial in Washington. So this is the soldier who doesn't die alone. There are two bodybags in there. One bag has him in it. Thereafter, he goes in the National War Memorial. He goes by himself — on a bender at the end of one song for the soldiers — he goes by himself in time, until he dies, is buried in time. And then people come to pay him a pleasant, happy, respectful obituary that's written up in the New York Times. So this goes on.\n",
            "\n",
            "Now there's another soldier who wasn't so happy. He's a war criminal. He got away in prison. So that's the second soldier in the book who goes in time. I won't say anything about him. He's a good-looking, solid American. He's from Chicago. He fought in Vietnam. As you can see, he writes a good-looking book.\n",
            "\n",
            "So it goes on. And on. And on.\n",
            "\n",
            "And now there it is. This is the soldier who didn't die alone. He writes a book about what he was like on his first day in prison. The second soldier is in the book, as well. But he writes it in two parts. He writes that he met this beautiful girl in prison. They had this one-and-a-half-hundred-dollar-a-month apartment. They had a great library. They had a television set. And they took the kids to the movies on weekends. So that's where the story began. In part one, they go off to the movies. And they have a blast. So it goes.\n",
            "\n",
            "And then the girl gives the boys a letter — not from her. It's from the prisoner's parents who live in a secluded part of the compound. In it, she tells them that a mistake they made a week before the girl's arrival has caused their son's death. It was one of those things that you make because the guards are always asking you about it. And so it goes on. And on.\n",
            "\n",
            "And then the kid is killed. And then the parents come by their house, see that kid's body, come back at a time that has to be caught on camera. They've got a camera ready for them, too. And they take a book out of the bookcase and show it to the parents. And the father thinks, What a lousy kid this kid is. He kills a kid, and he's going to be a star! So he tells this book-the child's story in the book. The kid comes back with this: I was the most wanted man in the city during my time there.\n",
            "\n",
            "Now the kid's dad looks at this book with horror. He can't believe his goddamnself. 'Was I a war criminal?' he says. 'Yes. I know about the books I read. They're all about the most wanted man in the city. They all have the same titles. \"The Most Wanted Man in the City — He's Dead!\"'\n",
            "\n",
            "The kid says, 'I know that a lot of the most wanted men in the city are killed by guns. But I also know a lot of them have books on them that they read when they aren't looking!'\n",
            "\n",
            "Now the dad comes in his own tiny car to the house, he says, 'How can you say all these nice things about a child you barely knew? What about those books he gave you? These are about the children who are the most wanted in this city? Those kids were your best friends?'\n",
            "\n",
            "Now the kid's mom is standing there, shaking in terror. 'You're lying to tell me that! You don't know what these are. It's just like telling your parents you were on the moon!' she said.\n",
            "\n",
            "Then\n",
            "\n",
            "[200 | 377.30] loss=2.26 avg=2.51\n",
            "[201 | 378.92] loss=2.79 avg=2.51\n",
            "[202 | 380.53] loss=2.07 avg=2.50\n",
            "[203 | 382.15] loss=2.07 avg=2.50\n",
            "[204 | 383.76] loss=2.37 avg=2.50\n",
            "[205 | 385.38] loss=2.24 avg=2.49\n",
            "[206 | 387.00] loss=2.43 avg=2.49\n",
            "[207 | 388.62] loss=2.27 avg=2.49\n",
            "[208 | 390.23] loss=1.92 avg=2.48\n",
            "[209 | 391.85] loss=2.08 avg=2.48\n",
            "[210 | 393.46] loss=1.87 avg=2.47\n",
            "[211 | 395.08] loss=2.46 avg=2.47\n",
            "[212 | 396.69] loss=2.66 avg=2.48\n",
            "[213 | 398.30] loss=2.48 avg=2.48\n",
            "[214 | 399.92] loss=2.98 avg=2.48\n",
            "[215 | 401.53] loss=2.61 avg=2.48\n",
            "[216 | 403.15] loss=2.07 avg=2.48\n",
            "[217 | 404.76] loss=2.63 avg=2.48\n",
            "[218 | 406.37] loss=1.77 avg=2.47\n",
            "[219 | 407.98] loss=2.13 avg=2.47\n",
            "[220 | 409.59] loss=1.92 avg=2.46\n",
            "[221 | 411.21] loss=2.90 avg=2.47\n",
            "[222 | 412.82] loss=2.55 avg=2.47\n",
            "[223 | 414.44] loss=1.97 avg=2.46\n",
            "[224 | 416.05] loss=2.21 avg=2.46\n",
            "[225 | 417.66] loss=2.73 avg=2.46\n",
            "[226 | 419.28] loss=2.23 avg=2.46\n",
            "[227 | 420.90] loss=2.59 avg=2.46\n",
            "[228 | 422.52] loss=2.37 avg=2.46\n",
            "[229 | 424.14] loss=1.92 avg=2.45\n",
            "[230 | 425.76] loss=2.28 avg=2.45\n",
            "[231 | 427.37] loss=2.43 avg=2.45\n",
            "[232 | 428.99] loss=1.76 avg=2.44\n",
            "[233 | 430.61] loss=2.33 avg=2.44\n",
            "[234 | 432.22] loss=1.98 avg=2.44\n",
            "[235 | 433.84] loss=1.70 avg=2.43\n",
            "[236 | 435.46] loss=2.01 avg=2.42\n",
            "[237 | 437.08] loss=2.36 avg=2.42\n",
            "[238 | 438.70] loss=2.23 avg=2.42\n",
            "[239 | 440.33] loss=1.72 avg=2.41\n",
            "[240 | 441.94] loss=2.46 avg=2.41\n",
            "[241 | 443.56] loss=2.20 avg=2.41\n",
            "[242 | 445.18] loss=1.84 avg=2.41\n",
            "[243 | 446.80] loss=1.77 avg=2.40\n",
            "[244 | 448.43] loss=2.22 avg=2.40\n",
            "[245 | 450.04] loss=2.10 avg=2.39\n",
            "[246 | 451.67] loss=2.38 avg=2.39\n",
            "[247 | 453.29] loss=1.96 avg=2.39\n",
            "[248 | 454.90] loss=2.69 avg=2.39\n",
            "[249 | 456.52] loss=2.49 avg=2.39\n",
            "[250 | 458.14] loss=2.29 avg=2.39\n",
            "[251 | 459.75] loss=2.13 avg=2.39\n",
            "[252 | 461.37] loss=2.36 avg=2.39\n",
            "[253 | 462.99] loss=1.75 avg=2.38\n",
            "[254 | 464.61] loss=2.50 avg=2.38\n",
            "[255 | 466.23] loss=1.72 avg=2.38\n",
            "[256 | 467.85] loss=2.05 avg=2.37\n",
            "[257 | 469.46] loss=2.59 avg=2.37\n",
            "[258 | 471.09] loss=2.02 avg=2.37\n",
            "[259 | 472.71] loss=2.03 avg=2.37\n",
            "[260 | 474.33] loss=2.55 avg=2.37\n",
            "[261 | 475.95] loss=2.26 avg=2.37\n",
            "[262 | 477.58] loss=2.14 avg=2.37\n",
            "[263 | 479.20] loss=1.93 avg=2.36\n",
            "[264 | 480.82] loss=2.17 avg=2.36\n",
            "[265 | 482.44] loss=2.35 avg=2.36\n",
            "[266 | 484.06] loss=2.49 avg=2.36\n",
            "[267 | 485.68] loss=1.65 avg=2.35\n",
            "[268 | 487.31] loss=2.11 avg=2.35\n",
            "[269 | 488.93] loss=2.51 avg=2.35\n",
            "[270 | 490.55] loss=2.21 avg=2.35\n",
            "[271 | 492.17] loss=2.53 avg=2.35\n",
            "[272 | 493.79] loss=2.41 avg=2.35\n",
            "[273 | 495.42] loss=2.01 avg=2.35\n",
            "[274 | 497.04] loss=2.37 avg=2.35\n",
            "[275 | 498.67] loss=2.52 avg=2.35\n",
            "[276 | 500.30] loss=2.29 avg=2.35\n",
            "[277 | 501.93] loss=2.23 avg=2.35\n",
            "[278 | 503.55] loss=1.95 avg=2.34\n",
            "[279 | 505.17] loss=2.08 avg=2.34\n",
            "[280 | 506.80] loss=2.09 avg=2.34\n",
            "[281 | 508.42] loss=2.54 avg=2.34\n",
            "[282 | 510.05] loss=2.02 avg=2.34\n",
            "[283 | 511.67] loss=2.25 avg=2.34\n",
            "[284 | 513.30] loss=1.69 avg=2.33\n",
            "[285 | 514.93] loss=2.01 avg=2.33\n",
            "[286 | 516.55] loss=2.15 avg=2.33\n",
            "[287 | 518.18] loss=1.95 avg=2.32\n",
            "[288 | 519.81] loss=1.88 avg=2.32\n",
            "[289 | 521.43] loss=1.67 avg=2.31\n",
            "[290 | 523.06] loss=2.44 avg=2.31\n",
            "[291 | 524.68] loss=1.88 avg=2.31\n",
            "[292 | 526.30] loss=1.83 avg=2.30\n",
            "[293 | 527.93] loss=1.76 avg=2.30\n",
            "[294 | 529.56] loss=2.97 avg=2.30\n",
            "[295 | 531.19] loss=1.67 avg=2.30\n",
            "[296 | 532.81] loss=1.73 avg=2.29\n",
            "[297 | 534.44] loss=2.02 avg=2.29\n",
            "[298 | 536.07] loss=2.16 avg=2.29\n",
            "[299 | 537.69] loss=1.86 avg=2.28\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "Pleasant', 'Warm' and 'Crisp', he's the epitome of what a young, healthy baby could look like.'\n",
            "\n",
            "For many months it looked like the children's father would starve to death before his son was old enough to walk.\n",
            "\n",
            "Then on one snowy Saturday night in late November, 1836, Charles Dickens found himself in the town where his father had died. Two of the children — a girl and her sister-in-law — had come to see him, and they were still playing around in the snow.\n",
            "\n",
            "All the while, the poor old writer was making phone calls between his home in Rochester, New York, and a number in Philadelphia. His phone rang. It was the dentist in Philadelphia. The dentist was calling from the mouth of the grave; in Philadelphia it was the grave of a former patient. The name of the patient was William Randolph Hearst. A number of people who knew and liked Randolph could not ignore the fact that Hearst had been one of the bloodiest and bloodiest mass murderers of all time, and that he was still in prison on a life sentence for the killings of a few dozen people in a day in the great summer resort of Palm Beach, Florida.\n",
            "\n",
            "The dentist knew that Hearst had a life sentence; that the time to serve would be swift and bitter if he broke it. The dentist could see that the thought of being his friend and business partner was tormenting Hearst; that the thought of being his friend and business partner was tormenting Hearst. So the line went from fear to pity to outrage to rage. It was all to no avail. The line opened over a voice of despair, and ended over death. The tone of the voice was: 'You had what I always ask men to have— a will to live. If you would have chosen life, you and I both would be dead. ... You have earned the opportunity to live.'\n",
            "\n",
            "In the middle of the tone of this voice was a sort of satisfaction. It was not satisfaction that life had a price. It was resignation that the man who had caused such pain had chosen to live. The man who had caused so much suffering had chosen to live.\n",
            "\n",
            "'Charles Dickens,' the voice concluded, 'I wish you good health.'\n",
            "\n",
            "And so on. And on. Every part of the voice was a joke. 'I wish you the greatest good luck,' it said, with such obvious, almost innocent amusement at the suffering of its victims that it was an obvious joke as well. And so on.\n",
            "\n",
            "Hearst had no time to respond. The voice on the other end of the line was so exasperated with Hearst's lack of effort that he was making this much further a reply: 'I also am sorry that you have chosen death, sir. You are much better off in our system of living, where no one is punished for any crime he or she may have committed.'\n",
            "\n",
            "And so on.\n",
            "\n",
            "There was a long moment in which the voice on the other end of the line did nothing but make a joke out of Hearst's lamentable state of health, which had been a joke up to that moment. It then said some more nonsense, and then said, 'It is my great wish that you, Charles, will live long enough so that, long after you are dead, you may become the happiest, healthiest man or woman you have ever seen — the kind of man or woman who will have chosen good health over any other vice or activity.'\n",
            "\n",
            "And it said these things to a man in the prison hospital. The voice on the other end of the line was still calling, asking Charles to reply. One of the things that the voice on the other end of the phone-extension line had noticed while answering it was that there was a difference in moods of those the caller thought might be about to die and those whom the caller did not. Thus the tone of the man-on-the-other-end of the line's answering machine was always intended to be such as to bring the two groups into close contact.\n",
            "\n",
            "There were things the man from the hospital could do for Hearst, and so on. But the end of the second line was silent. And then he had to wait. There was a silence as long as the earth around the sun. The next phone to the old writer was answered—and the last.\n",
            "\n",
            "\"Good morning,\" it said. One of the men on the other end told a story, and then the man on the other end told a story. The man still needed to be paid. The rest of the lines were silent. And then the telephone went dead.\n",
            "\n",
            "A man sitting next to the man who had answered the first line started speaking as soon as it went dead. A man from the hospital that day was the one who had come up with this phone extension. \"I thought you might like to know,' he said, 'that\n",
            "\n",
            "[300 | 561.36] loss=1.66 avg=2.27\n",
            "[301 | 562.97] loss=1.91 avg=2.27\n",
            "[302 | 564.59] loss=1.97 avg=2.27\n",
            "[303 | 566.21] loss=2.14 avg=2.27\n",
            "[304 | 567.82] loss=1.92 avg=2.26\n",
            "[305 | 569.44] loss=1.79 avg=2.26\n",
            "[306 | 571.05] loss=1.96 avg=2.25\n",
            "[307 | 572.66] loss=2.02 avg=2.25\n",
            "[308 | 574.27] loss=1.70 avg=2.25\n",
            "[309 | 575.88] loss=2.72 avg=2.25\n",
            "[310 | 577.49] loss=1.82 avg=2.25\n",
            "[311 | 579.10] loss=2.06 avg=2.25\n",
            "[312 | 580.71] loss=2.03 avg=2.24\n",
            "[313 | 582.32] loss=1.37 avg=2.23\n",
            "[314 | 583.93] loss=1.51 avg=2.23\n",
            "[315 | 585.54] loss=1.79 avg=2.22\n",
            "[316 | 587.15] loss=1.76 avg=2.22\n",
            "[317 | 588.77] loss=1.77 avg=2.21\n",
            "[318 | 590.38] loss=1.71 avg=2.21\n",
            "[319 | 592.00] loss=1.58 avg=2.20\n",
            "[320 | 593.61] loss=1.64 avg=2.19\n",
            "[321 | 595.23] loss=2.23 avg=2.19\n",
            "[322 | 596.85] loss=1.80 avg=2.19\n",
            "[323 | 598.47] loss=1.90 avg=2.19\n",
            "[324 | 600.08] loss=1.35 avg=2.18\n",
            "[325 | 601.70] loss=2.12 avg=2.18\n",
            "[326 | 603.32] loss=1.89 avg=2.18\n",
            "[327 | 604.94] loss=1.75 avg=2.17\n",
            "[328 | 606.56] loss=2.17 avg=2.17\n",
            "[329 | 608.18] loss=2.00 avg=2.17\n",
            "[330 | 609.80] loss=2.28 avg=2.17\n",
            "[331 | 611.42] loss=1.79 avg=2.17\n",
            "[332 | 613.04] loss=1.50 avg=2.16\n",
            "[333 | 614.67] loss=2.72 avg=2.17\n",
            "[334 | 616.28] loss=1.74 avg=2.16\n",
            "[335 | 617.91] loss=1.57 avg=2.15\n",
            "[336 | 619.53] loss=1.76 avg=2.15\n",
            "[337 | 621.15] loss=2.18 avg=2.15\n",
            "[338 | 622.77] loss=1.83 avg=2.15\n",
            "[339 | 624.39] loss=1.89 avg=2.14\n",
            "[340 | 626.02] loss=1.52 avg=2.14\n",
            "[341 | 627.63] loss=2.22 avg=2.14\n",
            "[342 | 629.26] loss=1.53 avg=2.13\n",
            "[343 | 630.88] loss=2.32 avg=2.13\n",
            "[344 | 632.50] loss=1.65 avg=2.13\n",
            "[345 | 634.12] loss=1.61 avg=2.12\n",
            "[346 | 635.75] loss=2.40 avg=2.13\n",
            "[347 | 637.37] loss=1.34 avg=2.12\n",
            "[348 | 638.99] loss=1.75 avg=2.12\n",
            "[349 | 640.61] loss=1.70 avg=2.11\n",
            "[350 | 642.23] loss=1.45 avg=2.10\n",
            "[351 | 643.85] loss=1.96 avg=2.10\n",
            "[352 | 645.48] loss=1.57 avg=2.10\n",
            "[353 | 647.10] loss=1.35 avg=2.09\n",
            "[354 | 648.72] loss=1.54 avg=2.08\n",
            "[355 | 650.35] loss=1.13 avg=2.07\n",
            "[356 | 651.96] loss=1.25 avg=2.07\n",
            "[357 | 653.59] loss=1.51 avg=2.06\n",
            "[358 | 655.21] loss=1.74 avg=2.06\n",
            "[359 | 656.84] loss=1.38 avg=2.05\n",
            "[360 | 658.46] loss=1.32 avg=2.04\n",
            "[361 | 660.08] loss=2.21 avg=2.04\n",
            "[362 | 661.70] loss=1.64 avg=2.04\n",
            "[363 | 663.32] loss=1.62 avg=2.04\n",
            "[364 | 664.95] loss=2.47 avg=2.04\n",
            "[365 | 666.57] loss=2.00 avg=2.04\n",
            "[366 | 668.19] loss=1.79 avg=2.04\n",
            "[367 | 669.81] loss=2.19 avg=2.04\n",
            "[368 | 671.43] loss=1.89 avg=2.04\n",
            "[369 | 673.05] loss=2.12 avg=2.04\n",
            "[370 | 674.67] loss=1.89 avg=2.04\n",
            "[371 | 676.30] loss=2.10 avg=2.04\n",
            "[372 | 677.92] loss=2.13 avg=2.04\n",
            "[373 | 679.54] loss=1.78 avg=2.04\n",
            "[374 | 681.16] loss=1.75 avg=2.03\n",
            "[375 | 682.77] loss=1.74 avg=2.03\n",
            "[376 | 684.40] loss=1.72 avg=2.03\n",
            "[377 | 686.02] loss=1.91 avg=2.03\n",
            "[378 | 687.65] loss=1.56 avg=2.02\n",
            "[379 | 689.27] loss=1.66 avg=2.02\n",
            "[380 | 690.89] loss=1.23 avg=2.01\n",
            "[381 | 692.51] loss=1.79 avg=2.01\n",
            "[382 | 694.13] loss=1.74 avg=2.00\n",
            "[383 | 695.75] loss=1.81 avg=2.00\n",
            "[384 | 697.37] loss=1.83 avg=2.00\n",
            "[385 | 699.00] loss=1.27 avg=1.99\n",
            "[386 | 700.62] loss=1.78 avg=1.99\n",
            "[387 | 702.24] loss=1.89 avg=1.99\n",
            "[388 | 703.86] loss=1.80 avg=1.99\n",
            "[389 | 705.47] loss=2.82 avg=2.00\n",
            "[390 | 707.09] loss=1.42 avg=1.99\n",
            "[391 | 708.71] loss=1.84 avg=1.99\n",
            "[392 | 710.32] loss=1.83 avg=1.99\n",
            "[393 | 711.94] loss=2.24 avg=1.99\n",
            "[394 | 713.56] loss=2.00 avg=1.99\n",
            "[395 | 715.18] loss=1.47 avg=1.98\n",
            "[396 | 716.80] loss=1.37 avg=1.98\n",
            "[397 | 718.42] loss=1.49 avg=1.97\n",
            "[398 | 720.03] loss=1.35 avg=1.97\n",
            "[399 | 721.65] loss=1.47 avg=1.96\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " time they all gathered, a black and white picture was show on the big screen. The picture showed a young woman in rags, her cheeks were puffy from crying, and her eyes were big and evil. An arrow stuck out of her forehead, stuck with her entire being. There in the picture were the initials of Satan and of his damned and damned-with-all marks. The young woman was Led Zeppelin. The world got the picture. Zeppelin had been the victim of a sex crime. \"Poor fella,\" said Billy, \"I'm so sorry. I know you weren't involved, but I just thought I'd let you know. And I'll keep that in mind here in hell, I guess.\" He said that he had a letter from his father, a letter that he was going to Hell anyway. Billy opened the envelope with his lips. In the interior were the signatures of four death row juries. So it goes. The headline read: 'DEATH ROW JUDGES DENY MOTORY CRUELTY TO 20 YEAR-OLD PERFECTTOGETHER LEE ZEPPELT.' The body of the letter said: 'My Dear Mr. Zeppelin: I'm sorry about the bad taste in your mouth (read: your heart), but I'm so thankful you're alive, for I wouldn't be a good father or husband or mother if I wasn't. If I were in your position, or even worse, dead, I would do the exact same thing. Now that you are dead I can tell you this: It's just as bad if not worse for you to get what you want as to get it at all. I have met so many people who said the same thing back to them after they were dead--that it was really so bad after death they were back in the world, living happily ever after. I guess they are. But I don't know. I guess it doesn't really bother them when everyone wants a piece of them, even though they are gone. I've just met this man --a very poor, almost mute old widower with poor health and no children --a widower who says that when he dies he never wants any part of the human race. His principal concern now is that the Earth be a very good place for his grandchildren. All he wants for his funeral is a lot of good cigars and a nice, sunny hour in the park. Poor old Billy Pilgrim never got to see the birth of his grandson. He probably never will.' 'I see the light now, my friend, I see the dawn now,' said Satan. 'O, I'm glad,' said Billy Pilgrim. 'You're always glad, do you? ' 'Yes,' said Satan. 'You're getting on so well there. You seem to be taking so long to get to Work. Well, then, it's time to get to Work, and I'm going to get to Work, and we'll see what happens. I thought you might like it this way: if it weren't for this lousy little letter I got from your father, I don't think I'd be here.' 'I like it that way,' said Billy. 'I think there's a nice symmetry to it. There's a part in you that believes everything you say, and a part that doesn't. That's the whole meaning of life. You gotta believe in 'em, or you ain't got nothing to live for. And I'm feeling pretty good right now in this. I've got a good job going. My pay's good. My health's good. My children're smart as fiddles. They'll all be alive to see the day when old Billy's a rich man again. ' 'You look good, Billy,' said Satan. 'It's a nice place, son, and I'm glad that your father let you in the club and that he took the trouble to write this letter. But I know his point of view. You know what I think, son? I think the same thing my father said: that it's all a big game, and that the first person to see to it that game's done is he who's to get to the bottom of it. He did, and he got to the bottom. If you can get old Zeppelin out of here, you can get everybody else out of here, too. But as I've said, I want you to believe in the devil and damnation and hell and good times and death and so on as a matter of right and justice and mercy and peace and so on, and I'm going to keep you there, as long as the hammer falls. ' 'You're right, son,' said Satan. 'I guess Billy's right, too. I guess I'm the bad guy here. We're all the bad guys there. I know Billy's right about that. I guess he's right about that too. I get on very well there. It's a beautiful place to be\n",
            "\n",
            "[400 | 746.30] loss=1.53 avg=1.96\n",
            "[401 | 747.92] loss=1.36 avg=1.95\n",
            "[402 | 749.53] loss=2.59 avg=1.96\n",
            "[403 | 751.15] loss=1.55 avg=1.95\n",
            "[404 | 752.76] loss=1.75 avg=1.95\n",
            "[405 | 754.38] loss=2.46 avg=1.96\n",
            "[406 | 756.00] loss=1.70 avg=1.95\n",
            "[407 | 757.62] loss=2.20 avg=1.96\n",
            "[408 | 759.23] loss=1.19 avg=1.95\n",
            "[409 | 760.85] loss=2.10 avg=1.95\n",
            "[410 | 762.47] loss=1.25 avg=1.94\n",
            "[411 | 764.09] loss=1.93 avg=1.94\n",
            "[412 | 765.70] loss=3.08 avg=1.95\n",
            "[413 | 767.31] loss=1.47 avg=1.95\n",
            "[414 | 768.93] loss=2.14 avg=1.95\n",
            "[415 | 770.54] loss=1.44 avg=1.95\n",
            "[416 | 772.15] loss=2.07 avg=1.95\n",
            "[417 | 773.77] loss=1.56 avg=1.94\n",
            "[418 | 775.39] loss=1.37 avg=1.94\n",
            "[419 | 777.00] loss=1.47 avg=1.93\n",
            "[420 | 778.61] loss=1.29 avg=1.93\n",
            "[421 | 780.23] loss=1.75 avg=1.92\n",
            "[422 | 781.84] loss=1.37 avg=1.92\n",
            "[423 | 783.46] loss=1.21 avg=1.91\n",
            "[424 | 785.08] loss=1.89 avg=1.91\n",
            "[425 | 786.69] loss=0.82 avg=1.90\n",
            "[426 | 788.30] loss=1.69 avg=1.90\n",
            "[427 | 789.92] loss=1.10 avg=1.89\n",
            "[428 | 791.53] loss=1.25 avg=1.88\n",
            "[429 | 793.15] loss=1.09 avg=1.88\n",
            "[430 | 794.76] loss=0.86 avg=1.87\n",
            "[431 | 796.38] loss=0.93 avg=1.86\n",
            "[432 | 798.00] loss=1.64 avg=1.85\n",
            "[433 | 799.61] loss=1.78 avg=1.85\n",
            "[434 | 801.23] loss=1.58 avg=1.85\n",
            "[435 | 802.85] loss=1.40 avg=1.85\n",
            "[436 | 804.46] loss=0.96 avg=1.84\n",
            "[437 | 806.08] loss=1.62 avg=1.83\n",
            "[438 | 807.69] loss=1.03 avg=1.83\n",
            "[439 | 809.30] loss=1.33 avg=1.82\n",
            "[440 | 810.92] loss=1.40 avg=1.82\n",
            "[441 | 812.53] loss=1.52 avg=1.81\n",
            "[442 | 814.15] loss=1.33 avg=1.81\n",
            "[443 | 815.76] loss=1.05 avg=1.80\n",
            "[444 | 817.38] loss=0.86 avg=1.79\n",
            "[445 | 819.00] loss=0.82 avg=1.78\n",
            "[446 | 820.61] loss=0.87 avg=1.77\n",
            "[447 | 822.22] loss=0.67 avg=1.76\n",
            "[448 | 823.84] loss=1.14 avg=1.76\n",
            "[449 | 825.46] loss=1.36 avg=1.75\n",
            "[450 | 827.07] loss=1.23 avg=1.75\n",
            "[451 | 828.69] loss=1.53 avg=1.74\n",
            "[452 | 830.30] loss=0.55 avg=1.73\n",
            "[453 | 831.92] loss=2.47 avg=1.74\n",
            "[454 | 833.53] loss=1.33 avg=1.74\n",
            "[455 | 835.15] loss=1.31 avg=1.73\n",
            "[456 | 836.77] loss=1.01 avg=1.72\n",
            "[457 | 838.38] loss=1.14 avg=1.72\n",
            "[458 | 840.00] loss=1.41 avg=1.72\n",
            "[459 | 841.62] loss=1.45 avg=1.71\n",
            "[460 | 843.24] loss=1.01 avg=1.71\n",
            "[461 | 844.85] loss=1.16 avg=1.70\n",
            "[462 | 846.47] loss=2.09 avg=1.70\n",
            "[463 | 848.09] loss=1.09 avg=1.70\n",
            "[464 | 849.70] loss=1.12 avg=1.69\n",
            "[465 | 851.32] loss=2.39 avg=1.70\n",
            "[466 | 852.93] loss=1.05 avg=1.69\n",
            "[467 | 854.56] loss=1.16 avg=1.69\n",
            "[468 | 856.18] loss=2.10 avg=1.69\n",
            "[469 | 857.79] loss=1.43 avg=1.69\n",
            "[470 | 859.40] loss=1.75 avg=1.69\n",
            "[471 | 861.02] loss=1.63 avg=1.69\n",
            "[472 | 862.64] loss=1.55 avg=1.69\n",
            "[473 | 864.25] loss=1.99 avg=1.69\n",
            "[474 | 865.87] loss=1.69 avg=1.69\n",
            "[475 | 867.48] loss=1.66 avg=1.69\n",
            "[476 | 869.10] loss=1.26 avg=1.69\n",
            "[477 | 870.72] loss=1.85 avg=1.69\n",
            "[478 | 872.33] loss=1.76 avg=1.69\n",
            "[479 | 873.94] loss=1.09 avg=1.68\n",
            "[480 | 875.56] loss=1.88 avg=1.68\n",
            "[481 | 877.18] loss=1.58 avg=1.68\n",
            "[482 | 878.80] loss=1.97 avg=1.69\n",
            "[483 | 880.41] loss=1.79 avg=1.69\n",
            "[484 | 882.03] loss=1.25 avg=1.68\n",
            "[485 | 883.65] loss=1.09 avg=1.68\n",
            "[486 | 885.27] loss=1.74 avg=1.68\n",
            "[487 | 886.88] loss=1.54 avg=1.68\n",
            "[488 | 888.49] loss=0.87 avg=1.67\n",
            "[489 | 890.11] loss=1.08 avg=1.66\n",
            "[490 | 891.72] loss=1.17 avg=1.66\n",
            "[491 | 893.34] loss=0.87 avg=1.65\n",
            "[492 | 894.95] loss=1.14 avg=1.64\n",
            "[493 | 896.57] loss=1.46 avg=1.64\n",
            "[494 | 898.20] loss=0.98 avg=1.64\n",
            "[495 | 899.81] loss=0.83 avg=1.63\n",
            "[496 | 901.43] loss=1.83 avg=1.63\n",
            "[497 | 903.05] loss=1.96 avg=1.63\n",
            "[498 | 904.67] loss=0.93 avg=1.63\n",
            "[499 | 906.28] loss=1.85 avg=1.63\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " with his life, he was a parakeet in a cage.\n",
            "\n",
            "The cage was surrounded by apple orchards. The cage was open at the top. Billy didn’t think the parakeets would see him. He wasn't expecting much.\n",
            "\n",
            "The parakeets circled him wildly and then they circled back at him. They didn't fear Billy for their shame. They feared the parrots fear.\n",
            "\n",
            "Billy could hear themselves talking. His voice was as clear as crystal. He was telling the truth just now—\n",
            "\n",
            "’I’d like to know the true name of my god’ —’I'd like to know the true name of my ...\n",
            "\n",
            "’My name is God,’ Billy answered.\n",
            "\n",
            "And then Billy saw that he was surrounded somehow by a mass of bright green smoke. Smoke that was about to climb the sides of the cage until it reached the top, but which was about to catch fire at the same time. And then it would burn, giving off a rainbow of sparks.\n",
            "\n",
            "'Tis as hot as the furnace, the smoke said. And it was. The sound was a whisper. Smoke and flame were good company. Billy was surrounded by loud boos, although he had a private channel open so that he could advise Roger Rabbit about the trouble with the fuses.\n",
            "\n",
            "’Ooooh, ooooooh,' sang one bird. ’Foolish man, fool ... ’ said another one. ’You shall not surely die.'\n",
            "\n",
            "'I think we are going to have to take refuge at the top of the cage,' said Billy Pilgrim.' 'There is nothing I like more than to see old men stuck in cages,' said Roger Rabbit, 'but there is no better place to hide a woman than at the top of the cage.’\n",
            "\n",
            "Soon the clanging of the trays stopped. The ovens were dark. 'I’ll take you up,’ said one. 'I want a box of cigarettes.' The little birds sang again. 'Here we go.' The parakeets circled and circled. They were all tucking their legs under them. They were all snug as velvet dolls. 'Thank you, thank you,' said one. ’It was awful,' said the other. And there were tables and chairs and a little supper and a clock. ’Five,’ said Billy. ’Eight,’ said Roger Rabbit, and they sang again. ’Eight,’ said Billy Pilgrim. And they were gone.\n",
            "\n",
            "Billy Pilgrim, who had not been home for Christmas Day, had gone to America on business. He was scheduled to appear as a guest on April Fools' Day, and, to amuse himself, he had compiled a long list of absurd imaginary people who might happen to live down the street from him. Billy had also drawn a map of the United States, showing where all the cities with population of twenty-five or more, according to the population register, might be found. He was sure that at least half of the cities on the list had been invented by him.\n",
            "\n",
            "Billy went to his hotel and, putting his business cards in the register, went to the door to see what the reception would be like. There was silence for fifteen minutes. The door was Western Union-card and all. Then it was played from silent to full volume. The message read in part:\n",
            "\n",
            "From: \"Gilles de Rais, Chief of the New York City Police Department, Special Investigations Bureau\"\n",
            "\n",
            "Subject: Interview with Billy Pilgrim.\n",
            "\n",
            "Body: 7 pages, black and white.\n",
            "\n",
            "Date: Thursday, April 27, 1985\n",
            "\n",
            "Hank Green, The Guardian, London, England\n",
            "\n",
            "The following interview was conducted on April 26, 1985. Interviewees will remain anonymous except as required by law. This interview has been transcribed for readers who wish to read it in their own words. The translators deeply apologize if any errors appear. Billy Pilgrim and I spoke for eight hours on the verandah of my building. I did not touch the birds for eight hours afterwards, and after that he never saw my hands again.\n",
            "\n",
            "I met Billy in the courtyard of this city slum on the afternoon of March 29. At five in the morning on a sunny day in the late fifties, a crowd of as many as two thousand people had gathered on the veranda of this low-ceilinged church. The Rev. John P. O'Hare was the minister. The crowd was out to celebrate the publication of a book about the Pink Flamingos. The Pink Flamingos, Reverend O'Hare explained, were poor children from Oklahoma who had come to New York to find work as entertainers. The book was about how the Pink Flamingos could be self-supporting, that they could buy houses, and so on. The book had\n",
            "\n",
            "[500 | 930.23] loss=1.72 avg=1.63\n",
            "[501 | 931.85] loss=1.31 avg=1.63\n",
            "[502 | 933.47] loss=1.70 avg=1.63\n",
            "[503 | 935.09] loss=1.28 avg=1.62\n",
            "[504 | 936.70] loss=1.31 avg=1.62\n",
            "[505 | 938.31] loss=1.52 avg=1.62\n",
            "[506 | 939.93] loss=0.67 avg=1.61\n",
            "[507 | 941.54] loss=1.50 avg=1.61\n",
            "[508 | 943.16] loss=1.65 avg=1.61\n",
            "[509 | 944.77] loss=1.07 avg=1.60\n",
            "[510 | 946.38] loss=1.00 avg=1.60\n",
            "[511 | 948.00] loss=1.03 avg=1.59\n",
            "[512 | 949.61] loss=0.94 avg=1.58\n",
            "[513 | 951.22] loss=0.71 avg=1.58\n",
            "[514 | 952.83] loss=1.37 avg=1.57\n",
            "[515 | 954.45] loss=1.18 avg=1.57\n",
            "[516 | 956.06] loss=0.66 avg=1.56\n",
            "[517 | 957.68] loss=1.43 avg=1.56\n",
            "[518 | 959.30] loss=1.45 avg=1.56\n",
            "[519 | 960.92] loss=1.49 avg=1.56\n",
            "[520 | 962.53] loss=1.43 avg=1.56\n",
            "[521 | 964.15] loss=1.42 avg=1.55\n",
            "[522 | 965.77] loss=0.68 avg=1.55\n",
            "[523 | 967.39] loss=1.36 avg=1.54\n",
            "[524 | 969.00] loss=1.13 avg=1.54\n",
            "[525 | 970.62] loss=1.13 avg=1.54\n",
            "[526 | 972.24] loss=0.93 avg=1.53\n",
            "[527 | 973.86] loss=0.47 avg=1.52\n",
            "[528 | 975.48] loss=1.35 avg=1.52\n",
            "[529 | 977.09] loss=1.55 avg=1.52\n",
            "[530 | 978.71] loss=1.04 avg=1.51\n",
            "[531 | 980.33] loss=1.48 avg=1.51\n",
            "[532 | 981.95] loss=0.92 avg=1.51\n",
            "[533 | 983.57] loss=0.47 avg=1.50\n",
            "[534 | 985.19] loss=1.05 avg=1.49\n",
            "[535 | 986.81] loss=1.12 avg=1.49\n",
            "[536 | 988.43] loss=0.94 avg=1.48\n",
            "[537 | 990.04] loss=1.16 avg=1.48\n",
            "[538 | 991.67] loss=0.70 avg=1.47\n",
            "[539 | 993.29] loss=1.16 avg=1.47\n",
            "[540 | 994.90] loss=0.92 avg=1.46\n",
            "[541 | 996.52] loss=0.45 avg=1.45\n",
            "[542 | 998.14] loss=1.14 avg=1.45\n",
            "[543 | 999.76] loss=1.96 avg=1.45\n",
            "[544 | 1001.39] loss=1.40 avg=1.45\n",
            "[545 | 1003.01] loss=1.17 avg=1.45\n",
            "[546 | 1004.64] loss=1.05 avg=1.45\n",
            "[547 | 1006.25] loss=0.36 avg=1.44\n",
            "[548 | 1007.87] loss=0.71 avg=1.43\n",
            "[549 | 1009.49] loss=1.03 avg=1.43\n",
            "[550 | 1011.11] loss=1.24 avg=1.42\n",
            "[551 | 1012.74] loss=0.91 avg=1.42\n",
            "[552 | 1014.36] loss=0.93 avg=1.41\n",
            "[553 | 1015.98] loss=0.97 avg=1.41\n",
            "[554 | 1017.60] loss=1.12 avg=1.41\n",
            "[555 | 1019.23] loss=1.87 avg=1.41\n",
            "[556 | 1020.86] loss=1.55 avg=1.41\n",
            "[557 | 1022.48] loss=0.73 avg=1.41\n",
            "[558 | 1024.10] loss=1.26 avg=1.40\n",
            "[559 | 1025.72] loss=1.04 avg=1.40\n",
            "[560 | 1027.34] loss=0.71 avg=1.39\n",
            "[561 | 1028.96] loss=0.51 avg=1.38\n",
            "[562 | 1030.59] loss=0.96 avg=1.38\n",
            "[563 | 1032.21] loss=1.32 avg=1.38\n",
            "[564 | 1033.83] loss=0.77 avg=1.37\n",
            "[565 | 1035.45] loss=1.12 avg=1.37\n",
            "[566 | 1037.07] loss=0.53 avg=1.36\n",
            "[567 | 1038.69] loss=0.74 avg=1.36\n",
            "[568 | 1040.32] loss=0.99 avg=1.35\n",
            "[569 | 1041.95] loss=1.05 avg=1.35\n",
            "[570 | 1043.58] loss=0.54 avg=1.34\n",
            "[571 | 1045.20] loss=1.01 avg=1.34\n",
            "[572 | 1046.83] loss=1.35 avg=1.34\n",
            "[573 | 1048.45] loss=1.19 avg=1.34\n",
            "[574 | 1050.07] loss=1.26 avg=1.34\n",
            "[575 | 1051.69] loss=1.29 avg=1.34\n",
            "[576 | 1053.31] loss=0.84 avg=1.33\n",
            "[577 | 1054.94] loss=0.66 avg=1.32\n",
            "[578 | 1056.56] loss=0.79 avg=1.32\n",
            "[579 | 1058.18] loss=1.31 avg=1.32\n",
            "[580 | 1059.80] loss=0.75 avg=1.31\n",
            "[581 | 1061.42] loss=1.35 avg=1.31\n",
            "[582 | 1063.05] loss=0.56 avg=1.31\n",
            "[583 | 1064.67] loss=1.37 avg=1.31\n",
            "[584 | 1066.30] loss=0.77 avg=1.30\n",
            "[585 | 1067.92] loss=2.05 avg=1.31\n",
            "[586 | 1069.54] loss=0.84 avg=1.30\n",
            "[587 | 1071.17] loss=2.33 avg=1.31\n",
            "[588 | 1072.79] loss=1.53 avg=1.32\n",
            "[589 | 1074.41] loss=0.63 avg=1.31\n",
            "[590 | 1076.03] loss=0.78 avg=1.30\n",
            "[591 | 1077.65] loss=0.75 avg=1.30\n",
            "[592 | 1079.27] loss=0.77 avg=1.29\n",
            "[593 | 1080.89] loss=0.51 avg=1.28\n",
            "[594 | 1082.52] loss=0.95 avg=1.28\n",
            "[595 | 1084.15] loss=0.89 avg=1.28\n",
            "[596 | 1085.77] loss=1.06 avg=1.28\n",
            "[597 | 1087.39] loss=0.48 avg=1.27\n",
            "[598 | 1089.01] loss=0.50 avg=1.26\n",
            "[599 | 1090.63] loss=0.50 avg=1.25\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Optanio: This is the last time I will ever fly an airplane. \n",
            "\n",
            "The stewardess: What is it, Zeke? \n",
            "\n",
            "Zoey Zee years: I was in the goddamn movie 'Morning in Lanao del \n",
            "Firenze,' and I ate three grapefruits and a box of meringues each in one day. \n",
            "\n",
            "The stewardess: Zeke, you know what gives me chills? \n",
            "\n",
            "Zoey Zee years: Fucking drugs. \n",
            "\n",
            "The stewardess: What if I told you there was a way to take your drugs, fucking \n",
            "place them in vaults throughout the airplane, and when those vaults became full, the \n",
            "police would come and take you out? \n",
            "\n",
            "Zoey Zee years: I dunno. Couldn't hack it, though. I guess I'll have to \n",
            "suffer like the rest of you. \n",
            "\n",
            "The stewardess left. She was a rock star. \n",
            "\n",
            "Now the Fonz was working in his dad's hardware store in Ilium, a few miles \n",
            "north of his home. His dad, a bricklayer's mate, never misspent a single Saturday in his \n",
            "life. 'So-and-so,' he would whisper to his son off-screen, 'uses to say to me, \"You know \n",
            "Well, you know that place on the Camino which serves Jack Daniels? \"You should go \n",
            "there.\"' \n",
            "\n",
            "So it goes. So good father,' said the young Fonz. 'So bad dad used to say to him, \"You know \n",
            "well, son? \"You should go there, boy. It gets very hot down there.\"' \n",
            "\n",
            "Fonz was making steady money. He wasn't crazy-earning nothing now, but still making a \n",
            "good living. He wasn't thinking about the war, either. It didn’t interest him too much, \n",
            "outside the shop, though. His daughter, Amelia, was a year older, and she was sick, and \n",
            "Fonz was busy being a bricklayer. \n",
            "\n",
            "So it goes. \n",
            "\n",
            "On his birthday a year after that, 1960, his father took Fonz out to dinner. He had \n",
            "become quite fond of his father-in-law. The meal had been quite nice, except for one thing-the waiter \n",
            "was having a raucous time. He told the poor man it was time to go, that he and his good friend \n",
            "Carlos Trachtenberg were being hired immediately in Jupiter. The man in charge of the job \n",
            "was obviously drunk. The man had a bad attitude when he was drunk. \n",
            "\n",
            "'Oh my God,' said Fonz. 'Are you fucking nuts?' \n",
            "\n",
            "'You and I can talk about this on the road,' said his father. 'Traveling's good for you. Travel's \n",
            "good for the eyes. When you get back, you can look into the pictures and think, That was a \n",
            "fucken long time ago. That was a while back now. Travel's good for you. Travel's good for your \n",
            "belly. And your head, that's where all the action is.’ \n",
            "\n",
            "'That sounds good,' said Fonz. \n",
            "\n",
            "And on and on it went. \n",
            "\n",
            "'Have you ever been to the movies?' said his father. \n",
            "\n",
            "'Hell no,' said Fonz. 'Not when I got those jiggly little eyeshine burns at the beginning. \n",
            "Didn't pay for them. The money was on me. Didn't want to pay for it. So I let it burn. \n",
            "And here I am, stuck in traffic in Chicago.' \n",
            "\n",
            "This was what Fonzi used to call a life. Now, however, he was seeing a barber. It \n",
            "was Carl's turn. He was in the process of being electrocuted by the ice cold barbershop \n",
            "feud. And he told his story. \n",
            "\n",
            "It was all part of a druidic experiment involving Fonzi and Carl. The druid was \n",
            "recruiting friends for an initiation into the mysteries of death and the universe and nature. \n",
            "Fonzi was a new person to the lodge. He did not know that death and the unending universe were \n",
            "excellent friends. They sometimes got in the way-especially if you were carrying a lot of weight. \n",
            "\n",
            "So it goes. \n",
            "\n",
            "The druid and Fonzi had always had a great time, even though the druid was addicted to the \n",
            "opiates he hypnotized Carl with. Carl, of course, was hooked. There was no turning back. \n",
            "\n",
            "\n",
            "\n",
            "Fonzi had\n",
            "\n",
            "[600 | 1114.31] loss=1.57 avg=1.26\n",
            "[601 | 1115.93] loss=1.10 avg=1.25\n",
            "[602 | 1117.55] loss=1.01 avg=1.25\n",
            "[603 | 1119.17] loss=0.62 avg=1.24\n",
            "[604 | 1120.79] loss=0.62 avg=1.24\n",
            "[605 | 1122.41] loss=0.37 avg=1.23\n",
            "[606 | 1124.02] loss=0.38 avg=1.22\n",
            "[607 | 1125.64] loss=1.17 avg=1.22\n",
            "[608 | 1127.26] loss=0.72 avg=1.22\n",
            "[609 | 1128.87] loss=1.01 avg=1.21\n",
            "[610 | 1130.49] loss=0.90 avg=1.21\n",
            "[611 | 1132.11] loss=0.38 avg=1.20\n",
            "[612 | 1133.72] loss=1.40 avg=1.20\n",
            "[613 | 1135.33] loss=1.44 avg=1.21\n",
            "[614 | 1136.95] loss=0.77 avg=1.20\n",
            "[615 | 1138.57] loss=0.83 avg=1.20\n",
            "[616 | 1140.19] loss=0.74 avg=1.19\n",
            "[617 | 1141.80] loss=0.34 avg=1.19\n",
            "[618 | 1143.41] loss=0.96 avg=1.18\n",
            "[619 | 1145.03] loss=0.95 avg=1.18\n",
            "[620 | 1146.64] loss=0.69 avg=1.18\n",
            "[621 | 1148.26] loss=0.47 avg=1.17\n",
            "[622 | 1149.87] loss=0.71 avg=1.16\n",
            "[623 | 1151.49] loss=1.30 avg=1.17\n",
            "[624 | 1153.11] loss=0.65 avg=1.16\n",
            "[625 | 1154.73] loss=0.82 avg=1.16\n",
            "[626 | 1156.34] loss=1.34 avg=1.16\n",
            "[627 | 1157.96] loss=1.43 avg=1.16\n",
            "[628 | 1159.58] loss=2.14 avg=1.17\n",
            "[629 | 1161.20] loss=1.93 avg=1.18\n",
            "[630 | 1162.82] loss=0.46 avg=1.17\n",
            "[631 | 1164.44] loss=1.34 avg=1.17\n",
            "[632 | 1166.05] loss=0.77 avg=1.17\n",
            "[633 | 1167.67] loss=1.26 avg=1.17\n",
            "[634 | 1169.30] loss=0.60 avg=1.16\n",
            "[635 | 1170.92] loss=1.04 avg=1.16\n",
            "[636 | 1172.54] loss=3.03 avg=1.18\n",
            "[637 | 1174.16] loss=0.83 avg=1.18\n",
            "[638 | 1175.77] loss=1.05 avg=1.18\n",
            "[639 | 1177.39] loss=0.79 avg=1.17\n",
            "[640 | 1179.01] loss=0.33 avg=1.16\n",
            "[641 | 1180.62] loss=0.68 avg=1.16\n",
            "[642 | 1182.24] loss=0.61 avg=1.15\n",
            "[643 | 1183.87] loss=0.88 avg=1.15\n",
            "[644 | 1185.49] loss=1.65 avg=1.16\n",
            "[645 | 1187.11] loss=0.86 avg=1.15\n",
            "[646 | 1188.73] loss=0.61 avg=1.15\n",
            "[647 | 1190.34] loss=0.58 avg=1.14\n",
            "[648 | 1191.96] loss=0.87 avg=1.14\n",
            "[649 | 1193.59] loss=1.11 avg=1.14\n",
            "[650 | 1195.21] loss=0.67 avg=1.14\n",
            "[651 | 1196.84] loss=0.75 avg=1.13\n",
            "[652 | 1198.46] loss=1.11 avg=1.13\n",
            "[653 | 1200.08] loss=0.60 avg=1.13\n",
            "[654 | 1201.70] loss=0.73 avg=1.12\n",
            "[655 | 1203.31] loss=0.23 avg=1.11\n",
            "[656 | 1204.93] loss=0.55 avg=1.11\n",
            "[657 | 1206.55] loss=0.32 avg=1.10\n",
            "[658 | 1208.18] loss=0.99 avg=1.10\n",
            "[659 | 1209.80] loss=0.59 avg=1.09\n",
            "[660 | 1211.42] loss=0.81 avg=1.09\n",
            "[661 | 1213.04] loss=0.70 avg=1.09\n",
            "[662 | 1214.66] loss=0.80 avg=1.08\n",
            "[663 | 1216.28] loss=0.40 avg=1.08\n",
            "[664 | 1217.90] loss=0.86 avg=1.07\n",
            "[665 | 1219.53] loss=0.75 avg=1.07\n",
            "[666 | 1221.15] loss=0.79 avg=1.07\n",
            "[667 | 1222.77] loss=0.64 avg=1.06\n",
            "[668 | 1224.40] loss=0.62 avg=1.06\n",
            "[669 | 1226.02] loss=0.70 avg=1.06\n",
            "[670 | 1227.64] loss=0.85 avg=1.05\n",
            "[671 | 1229.26] loss=0.60 avg=1.05\n",
            "[672 | 1230.88] loss=0.47 avg=1.04\n",
            "[673 | 1232.50] loss=0.14 avg=1.03\n",
            "[674 | 1234.12] loss=0.98 avg=1.03\n",
            "[675 | 1235.75] loss=0.93 avg=1.03\n",
            "[676 | 1237.37] loss=2.09 avg=1.04\n",
            "[677 | 1238.99] loss=0.97 avg=1.04\n",
            "[678 | 1240.61] loss=0.36 avg=1.04\n",
            "[679 | 1242.24] loss=0.85 avg=1.03\n",
            "[680 | 1243.86] loss=0.54 avg=1.03\n",
            "[681 | 1245.49] loss=0.36 avg=1.02\n",
            "[682 | 1247.11] loss=0.48 avg=1.02\n",
            "[683 | 1248.73] loss=1.11 avg=1.02\n",
            "[684 | 1250.36] loss=0.45 avg=1.01\n",
            "[685 | 1251.98] loss=1.17 avg=1.01\n",
            "[686 | 1253.60] loss=1.24 avg=1.02\n",
            "[687 | 1255.22] loss=0.45 avg=1.01\n",
            "[688 | 1256.85] loss=0.98 avg=1.01\n",
            "[689 | 1258.47] loss=0.41 avg=1.00\n",
            "[690 | 1260.10] loss=0.22 avg=1.00\n",
            "[691 | 1261.72] loss=0.60 avg=0.99\n",
            "[692 | 1263.34] loss=0.72 avg=0.99\n",
            "[693 | 1264.97] loss=1.72 avg=1.00\n",
            "[694 | 1266.59] loss=1.07 avg=1.00\n",
            "[695 | 1268.22] loss=0.70 avg=0.99\n",
            "[696 | 1269.84] loss=0.81 avg=0.99\n",
            "[697 | 1271.47] loss=0.78 avg=0.99\n",
            "[698 | 1273.10] loss=1.31 avg=0.99\n",
            "[699 | 1274.73] loss=0.17 avg=0.99\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " \n",
            "\n",
            "'I don’t think the Russians have figured out that you can’t talk to the dead. The Chinese\n",
            "\n",
            "have figured that out, and they've stopped doing it, too.' \n",
            "\n",
            "\n",
            "'That’s good,' said Billy, who was beginning to sound like a true Russian. \n",
            "\n",
            "'I like the deadlier sound you make,’ the doctor continued. 'I hate the lower part of the body. \n",
            "It shudders, contains itself, and begs to die. It doesn’t like hot beverages, or good food. It \n",
            "loves cold. It shudders when you give it something it doesn't want, or when it gets something \n",
            "it doesn’t want.' \n",
            "\n",
            "'It seems to like me a great deal,' said Billy Pilgrim, 'and I like to think I'm living through \n",
            "the eyes of an anesthesiologist.' The anesthesiologist looked at the patient, and then at the \n",
            "doctor, and then at the doctor's cane, and then at himself. \n",
            "\n",
            "'I saw you shiver,' said the patient, and he shuddered, and he talked about the dead. \n",
            "\n",
            "'I saw you shiver once. I felt like putting a pin in your leg. I think that made the dead laugh, \n",
            "because those who see a pin put a stop to a merry-go-round. The pin stopped the merry-go \n",
            "round, but not the looping loop. So, too, may the chuckles of doctors and nurses and patients \n",
            "and philosophers slow the rise of the dead and the fall of the living.' \n",
            "\n",
            "'That's very kind of you, that's very noble a thing to say,' said the patient. He looked at the anesthetic \n",
            "machine. 'You killed one off just now, didn’t you?' \n",
            "\n",
            "'Yes,' said the anesthesiologist. 'I put a week's worth into that one shivering, single \n",
            "shivering day. I'm sorry.' So it goes. \n",
            "\n",
            "'That was one of the shudders I use all the time.’ \n",
            "\n",
            "'I know,' said Billy. He was shivering just then, too. So was the doctor. Billy was \n",
            "\n",
            "\n",
            "67 shivering, too. He couldn’t take the pills, couldn’t take the Band-Aids, couldn’t drink, \n",
            "couldn’t smoke, couldn’t do the things the doctor and the patient could do, couldn’t \n",
            "listen intently to anyone who talked to them. \n",
            "\n",
            "\"What's the matter, young man,’ said the voice that was talking to you. \n",
            "\n",
            "Billy turned his head, saw his father standing there. Billy's father was standing there, too. \n",
            "Billy's father reached into his shirtfront and took out a pair of large, round, clipped \n",
            "glasses. They were blood-shot and sticky. Billy's father threw them to him with a \n",
            "glib gesture, and then replaced them with ragged, bloodstained overcoats. \n",
            "\n",
            "'You look like a man who has been kicked in the gut by a bullock cart,' said the voice \n",
            "from the kitchen. \n",
            "\n",
            "Billy shook his head. 'Not like a man at all,' he said. \n",
            "\n",
            "'Oh. . . . Oh my God,' said his mother as Billy got into the back of the \n",
            "family ox. Her other son, Johnnie, was beside her, trying to contain himself in \n",
            "a chair with a bloodied handkerchief under it. \n",
            "\n",
            "Billy sat down. His shirt was bloodshot. His pants were stained crimson. His ankles \n",
            "were blue. The only object that was warm under his overcoat were the stethoscope \n",
            "sticks. The wind was biting at the lace of the apron that covered the little stove. \n",
            "\n",
            "The doctor came in after a while. He had been called to the table, and he sat down \n",
            "on the edge of the vestibule between Billy and the doctor, so that he might have a \n",
            "look. Billy's father was no longer there to look after him. He was furious with Billy \n",
            "for sitting there and ignoring what the doctor had told him to do. He glared at Billy, \n",
            "slamming the telephone button as though it were made of tin. \n",
            "\n",
            "'Father- In Rome ,’ said a voice from the other end. \n",
            "\n",
            "Billy didn’t respond. He was still glaring. \n",
            "\n",
            "'Come on son,’ the voice said. 'Let's get this over with. This isn’t about Roman \n",
            "behavior.' \n",
            "\n",
            "Billy looked round at the others. Johnnie had\n",
            "\n",
            "[700 | 1298.43] loss=0.37 avg=0.98\n",
            "[701 | 1300.05] loss=0.41 avg=0.97\n",
            "[702 | 1301.67] loss=0.91 avg=0.97\n",
            "[703 | 1303.28] loss=0.63 avg=0.97\n",
            "[704 | 1304.89] loss=0.47 avg=0.96\n",
            "[705 | 1306.51] loss=0.76 avg=0.96\n",
            "[706 | 1308.12] loss=0.42 avg=0.96\n",
            "[707 | 1309.73] loss=0.56 avg=0.95\n",
            "[708 | 1311.34] loss=0.71 avg=0.95\n",
            "[709 | 1312.95] loss=0.55 avg=0.95\n",
            "[710 | 1314.57] loss=0.51 avg=0.94\n",
            "[711 | 1316.18] loss=1.39 avg=0.95\n",
            "[712 | 1317.79] loss=0.17 avg=0.94\n",
            "[713 | 1319.40] loss=0.36 avg=0.93\n",
            "[714 | 1321.02] loss=0.58 avg=0.93\n",
            "[715 | 1322.63] loss=0.54 avg=0.93\n",
            "[716 | 1324.25] loss=0.64 avg=0.92\n",
            "[717 | 1325.86] loss=0.40 avg=0.92\n",
            "[718 | 1327.49] loss=0.33 avg=0.91\n",
            "[719 | 1329.10] loss=0.67 avg=0.91\n",
            "[720 | 1330.72] loss=0.57 avg=0.91\n",
            "[721 | 1332.34] loss=0.76 avg=0.90\n",
            "[722 | 1333.96] loss=0.48 avg=0.90\n",
            "[723 | 1335.58] loss=0.79 avg=0.90\n",
            "[724 | 1337.20] loss=0.34 avg=0.89\n",
            "[725 | 1338.82] loss=0.33 avg=0.89\n",
            "[726 | 1340.44] loss=0.79 avg=0.89\n",
            "[727 | 1342.06] loss=0.38 avg=0.88\n",
            "[728 | 1343.68] loss=0.29 avg=0.88\n",
            "[729 | 1345.31] loss=0.39 avg=0.87\n",
            "[730 | 1346.93] loss=0.28 avg=0.87\n",
            "[731 | 1348.55] loss=1.24 avg=0.87\n",
            "[732 | 1350.17] loss=0.62 avg=0.87\n",
            "[733 | 1351.80] loss=0.50 avg=0.86\n",
            "[734 | 1353.42] loss=0.49 avg=0.86\n",
            "[735 | 1355.05] loss=0.55 avg=0.86\n",
            "[736 | 1356.68] loss=0.41 avg=0.85\n",
            "[737 | 1358.31] loss=0.56 avg=0.85\n",
            "[738 | 1359.94] loss=1.03 avg=0.85\n",
            "[739 | 1361.56] loss=0.49 avg=0.85\n",
            "[740 | 1363.18] loss=0.36 avg=0.84\n",
            "[741 | 1364.81] loss=0.58 avg=0.84\n",
            "[742 | 1366.43] loss=0.34 avg=0.83\n",
            "[743 | 1368.06] loss=0.28 avg=0.83\n",
            "[744 | 1369.68] loss=0.50 avg=0.83\n",
            "[745 | 1371.30] loss=0.88 avg=0.83\n",
            "[746 | 1372.93] loss=0.57 avg=0.82\n",
            "[747 | 1374.55] loss=0.37 avg=0.82\n",
            "[748 | 1376.17] loss=0.59 avg=0.82\n",
            "[749 | 1377.79] loss=0.67 avg=0.82\n",
            "[750 | 1379.42] loss=0.37 avg=0.81\n",
            "[751 | 1381.04] loss=0.99 avg=0.81\n",
            "[752 | 1382.66] loss=0.48 avg=0.81\n",
            "[753 | 1384.28] loss=0.29 avg=0.80\n",
            "[754 | 1385.91] loss=0.40 avg=0.80\n",
            "[755 | 1387.53] loss=0.21 avg=0.79\n",
            "[756 | 1389.16] loss=0.70 avg=0.79\n",
            "[757 | 1390.78] loss=0.60 avg=0.79\n",
            "[758 | 1392.40] loss=0.44 avg=0.79\n",
            "[759 | 1394.03] loss=0.34 avg=0.78\n",
            "[760 | 1395.64] loss=0.68 avg=0.78\n",
            "[761 | 1397.27] loss=0.70 avg=0.78\n",
            "[762 | 1398.89] loss=0.69 avg=0.78\n",
            "[763 | 1400.51] loss=0.59 avg=0.78\n",
            "[764 | 1402.14] loss=0.20 avg=0.77\n",
            "[765 | 1403.76] loss=0.74 avg=0.77\n",
            "[766 | 1405.37] loss=0.42 avg=0.77\n",
            "[767 | 1407.00] loss=0.50 avg=0.77\n",
            "[768 | 1408.62] loss=0.45 avg=0.76\n",
            "[769 | 1410.24] loss=1.63 avg=0.77\n",
            "[770 | 1411.86] loss=0.28 avg=0.77\n",
            "[771 | 1413.48] loss=0.56 avg=0.77\n",
            "[772 | 1415.11] loss=0.29 avg=0.76\n",
            "[773 | 1416.73] loss=0.60 avg=0.76\n",
            "[774 | 1418.35] loss=0.60 avg=0.76\n",
            "[775 | 1419.97] loss=0.26 avg=0.75\n",
            "[776 | 1421.58] loss=0.39 avg=0.75\n",
            "[777 | 1423.21] loss=0.86 avg=0.75\n",
            "[778 | 1424.82] loss=0.34 avg=0.75\n",
            "[779 | 1426.44] loss=0.20 avg=0.74\n",
            "[780 | 1428.06] loss=0.45 avg=0.74\n",
            "[781 | 1429.68] loss=0.32 avg=0.73\n",
            "[782 | 1431.30] loss=0.44 avg=0.73\n",
            "[783 | 1432.92] loss=0.37 avg=0.73\n",
            "[784 | 1434.54] loss=1.02 avg=0.73\n",
            "[785 | 1436.16] loss=0.44 avg=0.73\n",
            "[786 | 1437.78] loss=0.28 avg=0.72\n",
            "[787 | 1439.39] loss=0.27 avg=0.72\n",
            "[788 | 1441.01] loss=0.34 avg=0.71\n",
            "[789 | 1442.63] loss=0.49 avg=0.71\n",
            "[790 | 1444.25] loss=1.39 avg=0.72\n",
            "[791 | 1445.87] loss=1.02 avg=0.72\n",
            "[792 | 1447.49] loss=0.35 avg=0.72\n",
            "[793 | 1449.11] loss=0.32 avg=0.71\n",
            "[794 | 1450.73] loss=0.35 avg=0.71\n",
            "[795 | 1452.35] loss=0.44 avg=0.71\n",
            "[796 | 1453.97] loss=0.08 avg=0.70\n",
            "[797 | 1455.59] loss=0.61 avg=0.70\n",
            "[798 | 1457.21] loss=0.41 avg=0.70\n",
            "[799 | 1458.83] loss=0.34 avg=0.69\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " ghoved to a nearby telephone box; it was clanging. There was a dead end in the telephone book. \n",
            "\n",
            "Billy went on dying. He breathed on the cold steel wall of the steel chamber, breathed in and out, \n",
            "breathed rich, rich, rich. \n",
            "\n",
            "'Oh my God, Mike,' said the guards. 'Oh my God!’ \n",
            "\n",
            "There were voices from outside. They were complaining that the furnace was too hot. \n",
            "Outside voices were safety alarms. That was to say, inside people could be heard \n",
            "complaining that the chamber was dangerously low in the social scale of the universe. \n",
            "\n",
            "Outside people could be heard complaining, among other things, that the chamber was dangerously \n",
            "small. \n",
            "\n",
            "Outside people were annoyed that the chamber was fissioning at such a speed. Outside \n",
            "people were worried that the fissioning might bring about the end of the universe. Outside \n",
            "people were disappointed that fission didn’t work out that well. Outside people were \n",
            "grateful that the universe was still going on, because otherwise they wouldn’t be here. \n",
            "Inside people weremoaned the fact that the universe was ending, because otherwise \n",
            "they wouldn't be paid any more. Outside people were aping the characters on stage at the \n",
            "Endless Movie Festival in Vermont, where everybody died, got into a car with other \n",
            "fans, and drove south on the 401. \n",
            "\n",
            "They were rewarded with a view of the Kennedy Space Center from McLean, Virginia. \n",
            "Nearby was the Potomac River, which was swollen with bags of frozen \n",
            "white Olympic gold. The river was very nice in summer. \n",
            "\n",
            "Three guards rode in the back of the Cadillac. They were former cavalrymen. They \n",
            "used a kind of Pavlovian process to determine which guards were trustworthy and which weren’t. \n",
            "They trained it constantly, making sure that they always got the nod. \n",
            "\n",
            "The Cadillac stopped on a raised expanse of grass in front of a circle of \n",
            "frozen boulders. A guard climbed a spit and peered down into the grass. From there he \n",
            "could see the grass knoll below. From there he could see the lake below, four miles \n",
            "long and four feet high. And so on. The grass knoll was fenced off. \n",
            "\n",
            "The grass knoll was empty. There were no birds or other insects. The grass was \n",
            "brushed with cannonballs. \n",
            "\n",
            "Billy Pilgrim sat up in bed, tried to get some rest. The breakfast he had on \n",
            "that gorgeous Sunday in the Catskills, though, tasted like shrapnel. There wasn't \n",
            "any cereal or bagels, either. There wasn’t anything to hold him down, to make him sleepy. \n",
            "He'd had enough of war. \n",
            "\n",
            "\n",
            "\n",
            "There was a knock on the door. A man answered it. Billy didn’t recognise the man. \n",
            "He was short, with a torn blazer and faded jeans and a faded blazer and a \n",
            "trembling tan baseball cap. He looked like a Rorschach blot on a blimp. Billy \n",
            "wanted to hug him, to tell him how much you had to do with getting him to \n",
            "join the Army. He couldn’t read mindlessly to the point of insanity, however. \n",
            "He was an OŽC, an offshoot of the East German military intelligence agency. He \n",
            "passed away two days ago. He was buried quietly. There were no relatives, and \n",
            "Billy couldn't find out anything about his death except that it had been a long time \n",
            "since he'd seen the sunrise. \n",
            "\n",
            "Nobody was home. It was Billy's wedding night. Poor old Edgar Derby was out of \n",
            "honey, and his headache was making Billy dizzy. \n",
            "\n",
            "Billy closed his eyes. The Army knocked. \n",
            "\n",
            "There was a double door at the other end, and Billy went in. Somebody in white \n",
            "gloved his ear. \"Private, there's a visitor from outside.\" \n",
            "\n",
            "Billy went into the bedroom, closed the bedroom door behind him. The occupant of the \n",
            "bedroom was a private in the infamous Brown Shoe bomber raid of June 1944. He was a \n",
            "less-than-glad man, ecstatic that two months' pay had been missed by the Military.-the longest \n",
            "timely holiday of German prisoners of war. \n",
            "\n",
            "Billy did the unthinkable. He lay down on his bed, and he cried. Somebody opened the door. \n",
            "Without waiting for permission to leave the room, Billy went out onto the ill-ventilated landing strip \n",
            "of B-24s. He looked down across the flooded grassy plain below. The sun was\n",
            "\n",
            "[800 | 1482.58] loss=1.23 avg=0.70\n",
            "[801 | 1484.19] loss=0.47 avg=0.70\n",
            "[802 | 1485.81] loss=1.85 avg=0.71\n",
            "[803 | 1487.43] loss=0.49 avg=0.71\n",
            "[804 | 1489.05] loss=0.38 avg=0.70\n",
            "[805 | 1490.67] loss=0.62 avg=0.70\n",
            "[806 | 1492.28] loss=0.27 avg=0.70\n",
            "[807 | 1493.90] loss=0.60 avg=0.70\n",
            "[808 | 1495.51] loss=0.81 avg=0.70\n",
            "[809 | 1497.12] loss=0.38 avg=0.69\n",
            "[810 | 1498.73] loss=0.28 avg=0.69\n",
            "[811 | 1500.35] loss=0.51 avg=0.69\n",
            "[812 | 1501.96] loss=0.89 avg=0.69\n",
            "[813 | 1503.58] loss=0.80 avg=0.69\n",
            "[814 | 1505.19] loss=0.37 avg=0.69\n",
            "[815 | 1506.80] loss=0.75 avg=0.69\n",
            "[816 | 1508.41] loss=0.49 avg=0.69\n",
            "[817 | 1510.03] loss=0.57 avg=0.69\n",
            "[818 | 1511.65] loss=0.43 avg=0.68\n",
            "[819 | 1513.26] loss=0.35 avg=0.68\n",
            "[820 | 1514.87] loss=0.19 avg=0.68\n",
            "[821 | 1516.48] loss=0.32 avg=0.67\n",
            "[822 | 1518.10] loss=0.61 avg=0.67\n",
            "[823 | 1519.72] loss=0.32 avg=0.67\n",
            "[824 | 1521.33] loss=0.78 avg=0.67\n",
            "[825 | 1522.95] loss=0.39 avg=0.67\n",
            "[826 | 1524.57] loss=0.18 avg=0.66\n",
            "[827 | 1526.19] loss=0.48 avg=0.66\n",
            "[828 | 1527.80] loss=0.20 avg=0.65\n",
            "[829 | 1529.42] loss=0.23 avg=0.65\n",
            "[830 | 1531.04] loss=0.20 avg=0.65\n",
            "[831 | 1532.66] loss=0.40 avg=0.64\n",
            "[832 | 1534.28] loss=0.40 avg=0.64\n",
            "[833 | 1535.89] loss=0.18 avg=0.64\n",
            "[834 | 1537.51] loss=0.38 avg=0.63\n",
            "[835 | 1539.13] loss=0.21 avg=0.63\n",
            "[836 | 1540.74] loss=0.19 avg=0.63\n",
            "[837 | 1542.36] loss=0.35 avg=0.62\n",
            "[838 | 1543.98] loss=0.38 avg=0.62\n",
            "[839 | 1545.60] loss=0.25 avg=0.62\n",
            "[840 | 1547.22] loss=1.60 avg=0.63\n",
            "[841 | 1548.84] loss=0.48 avg=0.62\n",
            "[842 | 1550.46] loss=0.59 avg=0.62\n",
            "[843 | 1552.08] loss=0.39 avg=0.62\n",
            "[844 | 1553.69] loss=0.43 avg=0.62\n",
            "[845 | 1555.31] loss=0.59 avg=0.62\n",
            "[846 | 1556.93] loss=0.24 avg=0.62\n",
            "[847 | 1558.54] loss=0.68 avg=0.62\n",
            "[848 | 1560.16] loss=0.09 avg=0.61\n",
            "[849 | 1561.78] loss=0.55 avg=0.61\n",
            "[850 | 1563.40] loss=0.25 avg=0.61\n",
            "[851 | 1565.03] loss=0.11 avg=0.60\n",
            "[852 | 1566.65] loss=0.25 avg=0.60\n",
            "[853 | 1568.27] loss=0.39 avg=0.60\n",
            "[854 | 1569.89] loss=0.16 avg=0.59\n",
            "[855 | 1571.51] loss=0.51 avg=0.59\n",
            "[856 | 1573.13] loss=0.57 avg=0.59\n",
            "[857 | 1574.75] loss=0.24 avg=0.59\n",
            "[858 | 1576.37] loss=0.45 avg=0.59\n",
            "[859 | 1577.99] loss=0.79 avg=0.59\n",
            "[860 | 1579.62] loss=0.70 avg=0.59\n",
            "[861 | 1581.24] loss=0.27 avg=0.59\n",
            "[862 | 1582.86] loss=0.37 avg=0.58\n",
            "[863 | 1584.48] loss=0.30 avg=0.58\n",
            "[864 | 1586.11] loss=0.35 avg=0.58\n",
            "[865 | 1587.73] loss=0.23 avg=0.58\n",
            "[866 | 1589.35] loss=0.38 avg=0.57\n",
            "[867 | 1590.98] loss=0.16 avg=0.57\n",
            "[868 | 1592.61] loss=0.19 avg=0.57\n",
            "[869 | 1594.22] loss=0.15 avg=0.56\n",
            "[870 | 1595.84] loss=0.36 avg=0.56\n",
            "[871 | 1597.46] loss=0.69 avg=0.56\n",
            "[872 | 1599.08] loss=0.45 avg=0.56\n",
            "[873 | 1600.70] loss=0.23 avg=0.56\n",
            "[874 | 1602.33] loss=0.31 avg=0.55\n",
            "[875 | 1603.95] loss=0.39 avg=0.55\n",
            "[876 | 1605.58] loss=0.74 avg=0.55\n",
            "[877 | 1607.20] loss=0.67 avg=0.56\n",
            "[878 | 1608.83] loss=0.38 avg=0.55\n",
            "[879 | 1610.45] loss=0.85 avg=0.56\n",
            "[880 | 1612.07] loss=0.21 avg=0.55\n",
            "[881 | 1613.70] loss=0.56 avg=0.55\n",
            "[882 | 1615.32] loss=0.17 avg=0.55\n",
            "[883 | 1616.95] loss=0.50 avg=0.55\n",
            "[884 | 1618.57] loss=0.34 avg=0.55\n",
            "[885 | 1620.19] loss=0.40 avg=0.55\n",
            "[886 | 1621.82] loss=0.22 avg=0.54\n",
            "[887 | 1623.45] loss=0.29 avg=0.54\n",
            "[888 | 1625.07] loss=0.29 avg=0.54\n",
            "[889 | 1626.70] loss=0.83 avg=0.54\n",
            "[890 | 1628.32] loss=0.22 avg=0.54\n",
            "[891 | 1629.95] loss=0.24 avg=0.53\n",
            "[892 | 1631.58] loss=0.19 avg=0.53\n",
            "[893 | 1633.20] loss=0.23 avg=0.53\n",
            "[894 | 1634.82] loss=0.56 avg=0.53\n",
            "[895 | 1636.44] loss=0.52 avg=0.53\n",
            "[896 | 1638.07] loss=0.44 avg=0.53\n",
            "[897 | 1639.69] loss=0.29 avg=0.52\n",
            "[898 | 1641.32] loss=0.42 avg=0.52\n",
            "[899 | 1642.95] loss=0.73 avg=0.53\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " \n",
            "\n",
            "Billy and I talked for a while in the car. I told him I was going to kill him, and then I \n",
            "started to cry. \n",
            "\n",
            "\n",
            "\n",
            "'That's okay,' I told the radio. 'You can always come back and live with Mama and Dad \n",
            "in Ilium. They'll love you.' \n",
            "\n",
            "Billy laughed. 'Just remember-Mama and Dad are happy here. And everybody has a \n",
            "big adventure ahead of them.’ \n",
            "\n",
            "I told Billy to be careful, because I was going to make him miserable. \n",
            "\n",
            "That evening, Billy went to bed early, and then he was miserable all the way to work. \n",
            "He told his wife, and then he went home to his father-in-law's dairy farm. When he \n",
            "come down from the farm, he called his father-in-law, and he said to him, 'Father, \n",
            "\"Are you there?\" \n",
            "\n",
            "'Yes.' \n",
            "\n",
            "\"Oh my God, my gosh. Oh God, my gosh. Oh, my God. Oh God, my gosh.' \n",
            "\n",
            "He said, 'Are you there,’ and he indicated where his cottage was. \n",
            "\n",
            "'Yes.' \n",
            "\n",
            "\"Tell me where you've hidden that million dollars.' \n",
            "\n",
            "'I'm not hiding it,' said Billy. 'It's right here.' \n",
            "\n",
            "'Oh God, oh God.' \n",
            "\n",
            "'Tell me whatcha hiding’ \n",
            "\n",
            "'I don’t even know,' said Billy. 'I don’t even know whatI'm supposed to be hiding \n",
            "anymore.' \n",
            "\n",
            "'That's what I'm afraid of the most,' said the Dauphin. 'Losing one's head.' \n",
            "\n",
            "Billy was haunted by the question, ever haunted by the question: 'Are you there, Father?' \n",
            "\n",
            "Billy felt as though he were contemplating the fate of the thousands of other prisoners who had \n",
            "been arrested for the same offense, except that he himself had been arrested for stealing \n",
            "thousands of dollars. Billy had been suspected of stealing about a ton of jewelry. So it goes. \n",
            "\n",
            "Billy was shackled to the radiator of his iron chair, and the last thing he remembered was waking up in \n",
            "the iron chamber of a crematorium. \n",
            "\n",
            "He was blindfolded, and his eyes were swaddled in foil. An electric shock had \n",
            "harshly revealed what was really hidden in Billy's head. The shock was that of the one-punch \n",
            "in Detective Rust's belt. \n",
            "\n",
            "'Interesting,' said the Dauphin, 'that body doubles are such a dime a dozen.’ \n",
            "\n",
            "Billy was drenched in sweat. The freezing weather had nothing to do with the spectre haunting \n",
            "him-the thundering rain. The rain was an accessory-a side dish. It served to intensify \n",
            "Billy's already-impressive list of inflictes. The first thing that hit Billy, when he was \n",
            "dressed, was blindness. It was an ache in his right 24 1/2th of the sightless skull, which \n",
            "then became covered with sallow wrinkles. It took several hours for Billy to find a pair of \n",
            "bare feet that were both wooden, and which were, moreover, rough and rugged. They \n",
            "came from a byzantine land route that hadn’t been paved yet. And then, on the third \n",
            "day, Billy ran marathons, and won. In his war against the invisible hand of gravity, \n",
            "Billy was unstoppable. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Billy Pilgrim in the steel mill of Ilium, New Hampshire. Billy was three months old when\n",
            "he was put to bed against the wall of a metal-eating iron and glass toilet. \n",
            "The stall was empty. The poor creature wanted out. It was soaked with sweat, and it \n",
            "sloshed in and out of consciousnesses of incredible fluidity and freedom. It had been alive only \n",
            "for a split second, and then it was gone. Its memory had been a jumble, compressed so \n",
            "quickly into tiny lozenges. Now it was back in the stall again, with all those foolish foibles of \n",
            "life that had made life seem like such a pantomime. It had to be back in the pit again, \n",
            "where it had first gone to sleep. \n",
            "\n",
            "Billy's mother came in through the back door, called out that her son was in \n",
            "a fit of rage. She meant well. So it goes. She was pregnant with two more children, and her \n",
            "mind was full of important matters to discuss. And Billy, his back to her, pleaded with her to take \n",
            "off her shoes. \n",
            "\n",
            "[900 | 1667.60] loss=0.34 avg=0.52\n",
            "[901 | 1669.21] loss=0.35 avg=0.52\n",
            "[902 | 1670.82] loss=0.32 avg=0.52\n",
            "[903 | 1672.43] loss=0.20 avg=0.52\n",
            "[904 | 1674.04] loss=0.32 avg=0.51\n",
            "[905 | 1675.65] loss=0.26 avg=0.51\n",
            "[906 | 1677.26] loss=0.21 avg=0.51\n",
            "[907 | 1678.87] loss=0.41 avg=0.51\n",
            "[908 | 1680.49] loss=0.22 avg=0.51\n",
            "[909 | 1682.10] loss=0.31 avg=0.50\n",
            "[910 | 1683.71] loss=0.15 avg=0.50\n",
            "[911 | 1685.32] loss=0.18 avg=0.50\n",
            "[912 | 1686.94] loss=0.51 avg=0.50\n",
            "[913 | 1688.55] loss=0.15 avg=0.49\n",
            "[914 | 1690.17] loss=0.45 avg=0.49\n",
            "[915 | 1691.78] loss=0.25 avg=0.49\n",
            "[916 | 1693.39] loss=0.40 avg=0.49\n",
            "[917 | 1695.01] loss=0.34 avg=0.49\n",
            "[918 | 1696.63] loss=0.25 avg=0.49\n",
            "[919 | 1698.25] loss=0.53 avg=0.49\n",
            "[920 | 1699.87] loss=0.16 avg=0.48\n",
            "[921 | 1701.48] loss=0.23 avg=0.48\n",
            "[922 | 1703.10] loss=0.17 avg=0.48\n",
            "[923 | 1704.73] loss=0.28 avg=0.47\n",
            "[924 | 1706.35] loss=0.56 avg=0.48\n",
            "[925 | 1707.97] loss=0.28 avg=0.47\n",
            "[926 | 1709.59] loss=1.01 avg=0.48\n",
            "[927 | 1711.21] loss=0.19 avg=0.48\n",
            "[928 | 1712.83] loss=0.15 avg=0.47\n",
            "[929 | 1714.45] loss=0.37 avg=0.47\n",
            "[930 | 1716.07] loss=0.21 avg=0.47\n",
            "[931 | 1717.69] loss=0.16 avg=0.47\n",
            "[932 | 1719.31] loss=0.15 avg=0.46\n",
            "[933 | 1720.94] loss=0.29 avg=0.46\n",
            "[934 | 1722.56] loss=0.14 avg=0.46\n",
            "[935 | 1724.19] loss=0.16 avg=0.46\n",
            "[936 | 1725.81] loss=0.30 avg=0.45\n",
            "[937 | 1727.43] loss=0.36 avg=0.45\n",
            "[938 | 1729.05] loss=0.44 avg=0.45\n",
            "[939 | 1730.67] loss=0.32 avg=0.45\n",
            "[940 | 1732.30] loss=0.39 avg=0.45\n",
            "[941 | 1733.92] loss=0.20 avg=0.45\n",
            "[942 | 1735.55] loss=1.11 avg=0.45\n",
            "[943 | 1737.17] loss=0.19 avg=0.45\n",
            "[944 | 1738.79] loss=0.28 avg=0.45\n",
            "[945 | 1740.41] loss=0.14 avg=0.45\n",
            "[946 | 1742.03] loss=0.28 avg=0.45\n",
            "[947 | 1743.65] loss=0.15 avg=0.44\n",
            "[948 | 1745.27] loss=0.19 avg=0.44\n",
            "[949 | 1746.89] loss=0.25 avg=0.44\n",
            "[950 | 1748.51] loss=0.16 avg=0.44\n",
            "[951 | 1750.13] loss=0.24 avg=0.43\n",
            "[952 | 1751.75] loss=0.11 avg=0.43\n",
            "[953 | 1753.37] loss=0.30 avg=0.43\n",
            "[954 | 1755.00] loss=0.17 avg=0.43\n",
            "[955 | 1756.62] loss=0.66 avg=0.43\n",
            "[956 | 1758.23] loss=0.48 avg=0.43\n",
            "[957 | 1759.86] loss=0.30 avg=0.43\n",
            "[958 | 1761.48] loss=0.66 avg=0.43\n",
            "[959 | 1763.10] loss=0.17 avg=0.43\n",
            "[960 | 1764.72] loss=0.33 avg=0.43\n",
            "[961 | 1766.34] loss=0.21 avg=0.42\n",
            "[962 | 1767.97] loss=0.31 avg=0.42\n",
            "[963 | 1769.59] loss=0.17 avg=0.42\n",
            "[964 | 1771.20] loss=0.55 avg=0.42\n",
            "[965 | 1772.82] loss=0.12 avg=0.42\n",
            "[966 | 1774.44] loss=0.18 avg=0.42\n",
            "[967 | 1776.06] loss=0.17 avg=0.41\n",
            "[968 | 1777.68] loss=0.29 avg=0.41\n",
            "[969 | 1779.30] loss=0.36 avg=0.41\n",
            "[970 | 1780.92] loss=0.18 avg=0.41\n",
            "[971 | 1782.55] loss=0.26 avg=0.41\n",
            "[972 | 1784.17] loss=0.26 avg=0.41\n",
            "[973 | 1785.79] loss=0.56 avg=0.41\n",
            "[974 | 1787.41] loss=0.40 avg=0.41\n",
            "[975 | 1789.03] loss=0.16 avg=0.41\n",
            "[976 | 1790.65] loss=0.44 avg=0.41\n",
            "[977 | 1792.27] loss=0.21 avg=0.40\n",
            "[978 | 1793.88] loss=0.18 avg=0.40\n",
            "[979 | 1795.50] loss=0.21 avg=0.40\n",
            "[980 | 1797.12] loss=0.18 avg=0.40\n",
            "[981 | 1798.74] loss=0.23 avg=0.40\n",
            "[982 | 1800.36] loss=0.23 avg=0.39\n",
            "[983 | 1801.98] loss=0.24 avg=0.39\n",
            "[984 | 1803.60] loss=0.20 avg=0.39\n",
            "[985 | 1805.21] loss=0.50 avg=0.39\n",
            "[986 | 1806.84] loss=0.22 avg=0.39\n",
            "[987 | 1808.45] loss=0.25 avg=0.39\n",
            "[988 | 1810.07] loss=0.16 avg=0.39\n",
            "[989 | 1811.69] loss=0.32 avg=0.39\n",
            "[990 | 1813.31] loss=0.55 avg=0.39\n",
            "[991 | 1814.93] loss=0.12 avg=0.38\n",
            "[992 | 1816.54] loss=0.15 avg=0.38\n",
            "[993 | 1818.17] loss=0.22 avg=0.38\n",
            "[994 | 1819.78] loss=0.23 avg=0.38\n",
            "[995 | 1821.40] loss=0.41 avg=0.38\n",
            "[996 | 1823.02] loss=0.32 avg=0.38\n",
            "[997 | 1824.64] loss=0.25 avg=0.38\n",
            "[998 | 1826.26] loss=0.15 avg=0.38\n",
            "[999 | 1827.88] loss=0.67 avg=0.38\n",
            "Saving checkpoint/run1/model-1000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " screen had a hole in it! But she didn’t say anything, because Mrs. Norris was a dead ringer for her. Mr. Norris was silent, because he wanted to do the honors. He wanted to answer the questions from the TV. \n",
            "\n",
            "What happened to Billy's hat when he was killed? And what became of the belt and the bucket? \n",
            "\n",
            "The soldiers went into the recreation center and bought Billy's hat, his belt, his penny and his knife. They sold them at a profit to a man who was a veteran, and who sold them on at a loss, since the center was razed to the ground. \n",
            "\n",
            "\n",
            "\n",
            "The Purple And Gold didn’t burn down again until 1958, on Tralfamadore. \n",
            "\n",
            "In the crater of Tralfamadore, with its empty lots and dead people all around, sits the 70-foot-tall bronze Statue of Liberty. She was mated to that mighty mechanical being by a Superstition meister named Fred Zeiler. Zeiler was under house arrest in Vermont when he came unstuck in time. Zeiler traveled in time to the year 2134. He was thirty-three years old. He was pregnant with his eighteenth child. \n",
            "\n",
            "'Howdy, howdy,' said Mercy Custer from the other side of the Dulce, 'how could you not see by now? It's been on my nerves ever since the end of the war.’ \n",
            "\n",
            "Earthling and all, guests of the Tralfamadorians, was what they were called. They had just come off their first year as tourists, and they were already gnashing their teeth over what, in their estimation, would be their principal loss during the trip, namely, their memories. \n",
            "\n",
            "'You guys go on without me,' said Owen Zita Saturday, the truck driver. Sunday was Luna \n",
            "Preserver Miss California, who had made Billy do all the holding. 'Cause I'm gonnna be a real historian.' \n",
            "\n",
            "Owen Zita was sincere. He was planning a full-scale army at Dresden, with all the horses and guns and planes and tunnels and granaries and Benelux cooking sachets all by himself in a shed without food or water or love or money or internet, for less than the worth of a penny, possibly not even that. \n",
            "And so on. Miss California was wrong to fret. The recollection of her ruined house in Brooklyn with its many decorative lights went straight to the top of the war book, and Owen Zita read it with ecstasy. \n",
            "\n",
            "Owens got to Dresden about two in the afternoon. There were throngs of us on roller coasters which were filled with bottled water. We were taken to a dock at the port of Haina in the Kuriles River. There were twenty-three other dockworkers onsite, waiting to take us to sea. There were racks and racks of aluminum drawers and shutters and paneling and drawers and drawers and so on which were filled with canned goods. And then there were thirty German servicemen fixing cables and setting clocks and so on. They were tourists. They had just come from the front in the Argonne Forest in Illinois. They had been cut in half by a rocket-propelled grenade eighteen days earlier. They had both broken their torsos. So it goes. \n",
            "\n",
            "We were moved in pairs, with placards saying so and so. We were shown into individual barracks. There were twenty-two of us in each. There were benches for us. There was a fire escape that led to an airlock fourteen feet below the street level. There were compartments where we could take our clothes and magazines and books and so on. There was a fire extinguisher hung upon a branch. There was a fire escape over the water at the end of a staircase. The fire escape was locked from the inside. It was guarded by two fire trucks. The inside of the airlock was fumigated with white smoke. There was a view of the Rudolph Sallee Performance Ring and the Verrazano-Narrows Bridge over the Verona River. It was evening. The ring was a rendering of a spider's web in miniature. The spider was a female Mandrake bee. The bridge was suspended in spiderwebs of their own. \n",
            "Three days after I left New York for Dresden, I attended an evening reception given by Mr. O'Hare at the home of Joseph A. O'Hare, an atheist, in Lake Forest, Illinois. Mr. O'Hare was a member of the Bar Association of Chicago. Among those present were the following persons: Lazzaro, Bem, Derby, Fitzgerald, Garrity, Harvey, Hoffman, Jones, Laufman, McChesney, Newsome, O'Hare, Paterson, Pressburger, Robertson, Schellenberg, Shelly, Stewart\n",
            "\n",
            "[1000 | 1863.95] loss=0.25 avg=0.38\n",
            "[1001 | 1865.61] loss=0.13 avg=0.37\n",
            "[1002 | 1867.27] loss=0.39 avg=0.37\n",
            "[1003 | 1868.93] loss=0.43 avg=0.38\n",
            "[1004 | 1870.58] loss=0.18 avg=0.37\n",
            "[1005 | 1872.23] loss=0.33 avg=0.37\n",
            "[1006 | 1873.89] loss=0.19 avg=0.37\n",
            "[1007 | 1875.52] loss=0.44 avg=0.37\n",
            "[1008 | 1877.16] loss=0.41 avg=0.37\n",
            "[1009 | 1878.80] loss=0.11 avg=0.37\n",
            "[1010 | 1880.43] loss=0.18 avg=0.37\n",
            "[1011 | 1882.06] loss=0.26 avg=0.37\n",
            "[1012 | 1883.70] loss=0.23 avg=0.37\n",
            "[1013 | 1885.33] loss=0.22 avg=0.36\n",
            "[1014 | 1886.95] loss=0.72 avg=0.37\n",
            "[1015 | 1888.58] loss=0.18 avg=0.37\n",
            "[1016 | 1890.20] loss=0.08 avg=0.36\n",
            "[1017 | 1891.83] loss=1.17 avg=0.37\n",
            "[1018 | 1893.45] loss=0.34 avg=0.37\n",
            "[1019 | 1895.06] loss=0.53 avg=0.37\n",
            "[1020 | 1896.68] loss=0.15 avg=0.37\n",
            "[1021 | 1898.29] loss=0.18 avg=0.37\n",
            "[1022 | 1899.91] loss=0.20 avg=0.37\n",
            "[1023 | 1901.52] loss=0.44 avg=0.37\n",
            "[1024 | 1903.13] loss=0.29 avg=0.37\n",
            "[1025 | 1904.74] loss=0.14 avg=0.36\n",
            "[1026 | 1906.36] loss=0.37 avg=0.36\n",
            "[1027 | 1907.97] loss=0.33 avg=0.36\n",
            "[1028 | 1909.57] loss=0.18 avg=0.36\n",
            "[1029 | 1911.18] loss=0.32 avg=0.36\n",
            "[1030 | 1912.79] loss=0.25 avg=0.36\n",
            "[1031 | 1914.40] loss=0.23 avg=0.36\n",
            "[1032 | 1916.01] loss=0.31 avg=0.36\n",
            "[1033 | 1917.62] loss=0.25 avg=0.36\n",
            "[1034 | 1919.23] loss=0.18 avg=0.36\n",
            "[1035 | 1920.84] loss=0.22 avg=0.35\n",
            "[1036 | 1922.45] loss=0.11 avg=0.35\n",
            "[1037 | 1924.06] loss=0.24 avg=0.35\n",
            "[1038 | 1925.68] loss=0.22 avg=0.35\n",
            "[1039 | 1927.29] loss=0.12 avg=0.35\n",
            "[1040 | 1928.90] loss=0.14 avg=0.35\n",
            "[1041 | 1930.51] loss=0.30 avg=0.34\n",
            "[1042 | 1932.13] loss=0.15 avg=0.34\n",
            "[1043 | 1933.75] loss=0.21 avg=0.34\n",
            "[1044 | 1935.37] loss=0.29 avg=0.34\n",
            "[1045 | 1936.99] loss=0.35 avg=0.34\n",
            "[1046 | 1938.61] loss=0.15 avg=0.34\n",
            "[1047 | 1940.23] loss=0.13 avg=0.34\n",
            "[1048 | 1941.85] loss=0.21 avg=0.34\n",
            "[1049 | 1943.47] loss=0.06 avg=0.33\n",
            "[1050 | 1945.09] loss=0.17 avg=0.33\n",
            "[1051 | 1946.72] loss=0.16 avg=0.33\n",
            "[1052 | 1948.34] loss=0.13 avg=0.33\n",
            "[1053 | 1949.97] loss=0.11 avg=0.33\n",
            "[1054 | 1951.60] loss=0.15 avg=0.32\n",
            "[1055 | 1953.22] loss=0.16 avg=0.32\n",
            "[1056 | 1954.84] loss=0.18 avg=0.32\n",
            "[1057 | 1956.47] loss=0.24 avg=0.32\n",
            "[1058 | 1958.09] loss=0.15 avg=0.32\n",
            "[1059 | 1959.71] loss=0.28 avg=0.32\n",
            "[1060 | 1961.33] loss=0.15 avg=0.32\n",
            "[1061 | 1962.96] loss=0.14 avg=0.31\n",
            "[1062 | 1964.58] loss=0.63 avg=0.32\n",
            "[1063 | 1966.20] loss=0.27 avg=0.32\n",
            "[1064 | 1967.83] loss=0.16 avg=0.32\n",
            "[1065 | 1969.45] loss=0.14 avg=0.31\n",
            "[1066 | 1971.08] loss=0.17 avg=0.31\n",
            "[1067 | 1972.71] loss=0.21 avg=0.31\n",
            "[1068 | 1974.34] loss=0.18 avg=0.31\n",
            "[1069 | 1975.96] loss=0.23 avg=0.31\n",
            "[1070 | 1977.59] loss=0.19 avg=0.31\n",
            "[1071 | 1979.22] loss=0.35 avg=0.31\n",
            "[1072 | 1980.85] loss=0.23 avg=0.31\n",
            "[1073 | 1982.48] loss=0.36 avg=0.31\n",
            "[1074 | 1984.10] loss=0.22 avg=0.31\n",
            "[1075 | 1985.72] loss=0.19 avg=0.31\n",
            "[1076 | 1987.34] loss=0.28 avg=0.31\n",
            "[1077 | 1988.97] loss=0.13 avg=0.30\n",
            "[1078 | 1990.59] loss=0.16 avg=0.30\n",
            "[1079 | 1992.21] loss=0.16 avg=0.30\n",
            "[1080 | 1993.83] loss=0.19 avg=0.30\n",
            "[1081 | 1995.45] loss=0.20 avg=0.30\n",
            "[1082 | 1997.08] loss=0.36 avg=0.30\n",
            "[1083 | 1998.70] loss=0.13 avg=0.30\n",
            "[1084 | 2000.32] loss=0.14 avg=0.30\n",
            "[1085 | 2001.94] loss=0.10 avg=0.29\n",
            "[1086 | 2003.56] loss=0.15 avg=0.29\n",
            "[1087 | 2005.18] loss=0.07 avg=0.29\n",
            "[1088 | 2006.80] loss=0.39 avg=0.29\n",
            "[1089 | 2008.43] loss=0.12 avg=0.29\n",
            "[1090 | 2010.05] loss=0.21 avg=0.29\n",
            "[1091 | 2011.68] loss=0.17 avg=0.29\n",
            "[1092 | 2013.30] loss=0.16 avg=0.29\n",
            "[1093 | 2014.92] loss=0.36 avg=0.29\n",
            "[1094 | 2016.54] loss=0.09 avg=0.29\n",
            "[1095 | 2018.16] loss=0.27 avg=0.29\n",
            "[1096 | 2019.78] loss=0.16 avg=0.28\n",
            "[1097 | 2021.40] loss=0.14 avg=0.28\n",
            "[1098 | 2023.02] loss=0.12 avg=0.28\n",
            "[1099 | 2024.65] loss=0.27 avg=0.28\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". \n",
            "\n",
            "I had to pee. \n",
            "\n",
            "I asked my husband what I should wear. \n",
            "\n",
            "He had some advice. He told me to dress conservatively. Remember, he said, \n",
            "that this is America, not the winter days of '37. Remember to dress like a common \n",
            "barber, or you'll look like a buzz kill. \n",
            "\n",
            "I reminded myself that I was America and that I was in the Army of 1811. \n",
            "\n",
            "We went into my husband's coat-still in its shearling trim-and I showed him my Mother Goose vest and \n",
            "boycotted dinner. \n",
            "\n",
            "When we got back from dinner, my husband and I were bedraggled and in terrible debt. \n",
            "We tried to figure out what had happened to our beards. \n",
            "\n",
            "'Barber shop kid,' he said. \n",
            "\n",
            "We tried another one. \n",
            "\n",
            "\"Oh my God, my God,' he said. \n",
            "\n",
            "'Are you okay?' \n",
            "\n",
            "\"Yeah.\" \n",
            "\n",
            "\"Were you in a movie? Did you like it?\" \n",
            "\n",
            "This time his poker face turned grave. \"Are you serious? \n",
            "\n",
            "\"Are you kidding? Are you serious? \n",
            "\n",
            "\"Did you ever think about killing everybody on this ship and becoming a star and \n",
            "living like a star and fixing things and \n",
            "quitting smoking and taking care of your family?\" \n",
            "\n",
            "\"Nope.\" \n",
            "\n",
            "\"Good. Now just leave the rest to me. I'm the master. \n",
            "\n",
            "\"\n",
            "\n",
            "We gave up on that. We didn’t have the faintest idea what kind of a man Brian could \n",
            "be. \n",
            "\n",
            "'What kind of a man?' \n",
            "\n",
            "Suddenly, concern blossomed in his eyes. \"What do you think a star's worth, ’ he asked \n",
            "Jeff. ' Three hundred dollars an hour? Four, five, six hundred? ' he asked Sarandon \n",
            "if she knew. \n",
            "\n",
            "Sarandon shook her head. 'No,' she said. 'A year worth of classes taught by a star, that's worth \n",
            "about a million dollars.’ So it goes. \n",
            "\n",
            "At dinner that night, Jeff Bridges said things that made Sarandon happy. 'You know,' she \n",
            "said, 'I've had men approach me after I've had accidents who've been rooting through my divorce \n",
            "cases just to talk to me. It turns out they work for Her Majesty's Government. I tell them, \n",
            "and I guarantee you, they're stumped. \"They're thinking, \"Who the hell is Margaret Sanger?’’ And \n",
            "then they're wannabes.’' \n",
            "\n",
            "\n",
            "\n",
            "Sarandon visited Jeff and Barbara Derby at their home in Ilium the next day. The \n",
            "women greeted her courteously. They were in bad shape, since their only rest day in \n",
            "the war had been twenty-four-hour stretches among the frozen tundra of Montana. Their \n",
            "bum cells were \n",
            "\n",
            "full of infections caused by bereaved parents who couldn’t take care of their children any more. \n",
            "\n",
            "Sarandon gave them food and water and a new pair of round spectacles she had found in \n",
            "the Garvey library. She had gotten them for free from a candidate. \n",
            "\n",
            "The couple greeted the Rangers with kisses on the cheek, told them they were warm and \n",
            "smiling faces in hell. \n",
            "\n",
            "Sarandon opened the front door and let the chill of the night in beat her skin. She \n",
            "went into the kitchen and pulled the couple's black Cadillac Eldorados from its storage \n",
            "room, put a quarter into each car. Then she called out, 'Press—go!' \n",
            "\n",
            "Three of the black Cadillac Eldorados—with their decals off—got out of their \n",
            "staging areas and came at her from all directions. And go they went. In the chaos, \n",
            "one of the white Eldorados struck Pam Grier in the ribs with a hoot. Grier was flying \n",
            "from the blow, speeding away from the punch, going from one end of the Cadillac to the \n",
            "other. The Cadillac slammed into her from behind, knocked her unconscious. \n",
            "\n",
            "Sarandon didn’t know what to make of all the sudden appearing in her living room. \n",
            "She had moved to a new house just before the accident, and she hadn’t heard \n",
            "her friends complaining about the barking of the farmhouse dog. So instead of going to \n",
            "work, she had gone to a convention as usual. There were plenty of sights to see, foods to \n",
            "digest, drinks to take, matches to light, gloves to keep the leather jacket \n",
            "\n",
            "\n",
            "[1100 | 2048.61] loss=0.07 avg=0.28\n",
            "[1101 | 2050.23] loss=0.18 avg=0.28\n",
            "[1102 | 2051.84] loss=0.24 avg=0.28\n",
            "[1103 | 2053.46] loss=0.27 avg=0.28\n",
            "[1104 | 2055.08] loss=0.52 avg=0.28\n",
            "[1105 | 2056.69] loss=0.12 avg=0.28\n",
            "[1106 | 2058.30] loss=0.17 avg=0.28\n",
            "[1107 | 2059.91] loss=0.12 avg=0.28\n",
            "[1108 | 2061.52] loss=0.25 avg=0.28\n",
            "[1109 | 2063.14] loss=0.10 avg=0.27\n",
            "[1110 | 2064.75] loss=0.21 avg=0.27\n",
            "[1111 | 2066.36] loss=0.18 avg=0.27\n",
            "[1112 | 2067.97] loss=0.18 avg=0.27\n",
            "[1113 | 2069.59] loss=0.07 avg=0.27\n",
            "[1114 | 2071.20] loss=0.25 avg=0.27\n",
            "[1115 | 2072.82] loss=0.52 avg=0.27\n",
            "[1116 | 2074.44] loss=0.17 avg=0.27\n",
            "[1117 | 2076.05] loss=0.35 avg=0.27\n",
            "[1118 | 2077.67] loss=0.30 avg=0.27\n",
            "[1119 | 2079.28] loss=0.18 avg=0.27\n",
            "[1120 | 2080.90] loss=0.13 avg=0.27\n",
            "[1121 | 2082.52] loss=0.14 avg=0.27\n",
            "[1122 | 2084.13] loss=0.28 avg=0.27\n",
            "[1123 | 2085.75] loss=0.16 avg=0.27\n",
            "[1124 | 2087.36] loss=0.16 avg=0.27\n",
            "[1125 | 2088.98] loss=0.28 avg=0.27\n",
            "[1126 | 2090.60] loss=0.29 avg=0.27\n",
            "[1127 | 2092.22] loss=0.16 avg=0.27\n",
            "[1128 | 2093.83] loss=0.22 avg=0.26\n",
            "[1129 | 2095.46] loss=0.34 avg=0.27\n",
            "[1130 | 2097.07] loss=0.15 avg=0.26\n",
            "[1131 | 2098.69] loss=0.15 avg=0.26\n",
            "[1132 | 2100.31] loss=0.11 avg=0.26\n",
            "[1133 | 2101.93] loss=0.23 avg=0.26\n",
            "[1134 | 2103.55] loss=0.18 avg=0.26\n",
            "[1135 | 2105.17] loss=0.15 avg=0.26\n",
            "[1136 | 2106.79] loss=0.17 avg=0.26\n",
            "[1137 | 2108.41] loss=0.10 avg=0.26\n",
            "[1138 | 2110.02] loss=0.19 avg=0.26\n",
            "[1139 | 2111.64] loss=0.25 avg=0.26\n",
            "[1140 | 2113.26] loss=0.14 avg=0.26\n",
            "[1141 | 2114.89] loss=0.08 avg=0.25\n",
            "[1142 | 2116.51] loss=0.34 avg=0.25\n",
            "[1143 | 2118.13] loss=0.18 avg=0.25\n",
            "[1144 | 2119.75] loss=0.10 avg=0.25\n",
            "[1145 | 2121.37] loss=0.11 avg=0.25\n",
            "[1146 | 2122.99] loss=0.15 avg=0.25\n",
            "[1147 | 2124.62] loss=0.19 avg=0.25\n",
            "[1148 | 2126.24] loss=0.25 avg=0.25\n",
            "[1149 | 2127.86] loss=0.08 avg=0.25\n",
            "[1150 | 2129.49] loss=0.09 avg=0.25\n",
            "[1151 | 2131.11] loss=0.18 avg=0.25\n",
            "interrupted\n",
            "Saving checkpoint/run1/model-1152\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owz3WFRlZuG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/ /content/drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAC5jXsKZy1r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "8dbbda58-fb19-4fd8-a280-350ba887061f"
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/run1/fetch /content/gpt-2/models/117M/"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/gpt-2/checkpoint/run1/fetch': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40CR4Cp_aZrE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1516
        },
        "outputId": "90078c6a-13e2-44e6-913a-05fc0095d35b"
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --top_k 40 --model_name \"345M\""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-25 06:04:07.814936: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-25 06:04:07.815180: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2a54680 executing computations on platform Host. Devices:\n",
            "2019-05-25 06:04:07.815212: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-25 06:04:08.062869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-25 06:04:08.063364: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2a53fa0 executing computations on platform CUDA. Devices:\n",
            "2019-05-25 06:04:08.063394: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-25 06:04:08.063765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-25 06:04:08.063789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-25 06:04:08.544208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-25 06:04:08.544275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-25 06:04:08.544289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-25 06:04:08.544596: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-05-25 06:04:08.544647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Model prompt >>> What is meaning of life?\n",
            "2019-05-25 06:04:35.605771: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "======================================== SAMPLE 1 ========================================\n",
            " Meaninglessness: the inability to form a meaning in the world. Meaninglessness is a concept that leads to death, as explained last week on The Christian Post. However, the idea is not completely new to humans.\n",
            "\n",
            "During the ancient Roman Empire, one Roman emperor called Lucretius described a life of meaninglessness:\n",
            "\n",
            "No one can give me more meaning than myself, for I do not know any others besides myself, but, as I have seen that no one else understands me well either, I have no choice in this respect, for the only person I can truly trust is myself. And I must go on living as my own thing. I have forgotten how to live, and yet, knowing that no one else has seen through my lies, I must continue to be my own man through the medium of men. [1]\n",
            "\n",
            "These teachings have since fallen to Earth, but the notion is a powerful one. In his famous book, The Meaning of Life, psychologist Philip Tetlock has developed numerous experiments and studies that give an intuitive account of what it means to live. He argues that meaning is tied to emotions such as love and pleasure (\"The basic basis of meaning is love.\"); empathy in the form of pity (\"The basic basis of feeling is pity.\"); and pleasure in the form of appreciation (\"The basic base of pleasure is admiration.\"). Together, these explain why so many individuals enjoy \"living their meaning.\" There is meaning, but also pain and the pain of no satisfaction. All of that can be found on some websites and podcasts, but the key is taking control of your own story, and seeing it reflected in the physical world.\n",
            "\n",
            "Therein lies the tragedy of meaninglessness: it brings you to the very end of life and puts you at the mercy of your own mind. There's a reason the Bible gives us the word \"bitter\" to describe suffering in this sense: it has something to lose for humanity to be capable (as humans have always been) of finding a meaning in the world. So, if you find that most of the days in your life have felt meaningless, this means that you have lost the ability to form meaning by using your mind as a means for communication.\n",
            "\n",
            "In a few days, the last of the light will come into your life, and as you look forward to that day when you see it coming, the thoughts within and the things you do will have changed, but the truth that you are still here at the very end of your existence will remain\n",
            "================================================================================\n",
            "Model prompt >>> Billy Pilgrim sat alone at home.\n",
            "======================================== SAMPLE 1 ========================================\n",
            " The story was written and illustrated by Bob McLeod, the author of Tales of Arcadia, a science fiction and fantasy adventure novel from 1987. The book covers the history of the characters: Pilgrim is a young man trapped in a city called Arcadia. Arcadia is the epicenter of mankind's future as they work toward a grand goal of unification in their universe; after their victory, their goal becomes the freedom for other people. Pilgrim and his crew of four allies embark on a journey to find the secret of Arcadia's origins, finding the secret to becoming the most powerful city ever. Pilgrim is forced to team up with other heroes, known as heroes of the city, to bring to fruition the ultimate goal; the destruction of Arcadia. The book was released on May 13, 1991, one month after the release of the movie Aliens.\n",
            "\n",
            "\n",
            "The book was published under the Omniverse publishing company that merged with Fantagraphics Publishing in 1994. Omniverse is the name of the publishing house under which Omniverse Books ran and under which Fantagraphics published The Book of the New Sun. The books featured at the time were The Adventures of Trigon, the tale of Trigon and his crew (which also featured the novel's hero, Kain), and The Star-Spangled Banner, a story set during the War of the Worlds.<|endoftext|>For those who've grown up with a passion for food, this year may be too perfect for you. Not surprisingly, this year also marks the 40th anniversary of our very first restaurant menu. So with so many things we were looking forward to, how did you decide on what to try? Our kitchen team was on hand to give us a brief tour of our new restaurant, to offer some tasty recommendations of restaurants that made a difference to you in the past, and to explain what a delicious dinner at our new restaurant means today.\n",
            "\n",
            "One big surprise for us was the amazing array of craft beers available—just as we did in our 20th year of running The Great Wall Cafe and Bar. So, in the meantime, you'll also find dozens of wines and whiskeys available for you to enjoy on tap at our new restaurant. We'd also like to recognize the staff who are now serving our customers at all of our restaurants, and encourage you to keep them in mind as you explore our new menu.\n",
            "\n",
            "And when you do find what you're looking for, feel free to give our server of the month, Erin, suggestions of local places to grab some lunch if you\n",
            "================================================================================\n",
            "Model prompt >>> You might want to take the stairs to the left.\n",
            "======================================== SAMPLE 1 ========================================\n",
            " If you make this step, you'll be taken to a room with a door. If you look carefully, it's a treasure chest. Go on through, and on the second floor, you'll run into a room with a bed on a bed. This is where your first encounter occurs: the Black Guard's Room. If I weren't so confused, I wouldn't have known to look for this before. The Black Guard is standing at the opposite wall to the right of the bed so I'll assume you want the same room as the Red Guard - he's right there on the bedside table. Pick up the treasure you'll soon find. Enter the room - your reward is a few souls and a sword. Return to Sera and make your way out to the right (you'll have to wait for another player to follow). The Black Guard is the guy facing you. Don't bother telling him where the treasure was - just approach him and ask him who wants it. If he thinks you did it right he'll admit defeat and you'll get a small reward. I chose to accept it and ask about the reward. If you tell him you don't know if you've won yet, he'll admit defeat and reward you 200 souls. Head straight for the left door - go through it and look over the chests to open the door to the right. Here's where you fight with The Black Guard - his attack has you falling into the pit of despair. Now you enter a room with a table. At the right-hand side of the table is a weapon. Go up to the wall and use the sword on it. At this point, turn left and go down the stairs to the left before proceeding further on. You'll encounter a fight nearby. Use the treasure on the table to defeat him and then move on to the next room.\n",
            "\n",
            "Note: This is where the Black Guard is standing right next to a chest. The Black Guard is standing next to an ocarina key in front of the Black Guard's room, which you'll use to reach the treasure chests you have left for, before continuing on to find The Black Guard in the throne room. Once you do that, you'll be taken to the next door. If you go back out, you'll get a reward on the right. Go on down the stairs from the throne room. Go through to find yourself in a room with two chests inside: one has the treasure and one has some weapons. Head up the ladder and up the stairs on the\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5253, in get_controller\n",
            "    yield g\n",
            "  File \"src/interactive_conditional_samples.py\", line 71, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 89, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 127, in Fire\n",
            "    component_trace = _Fire(component, args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 366, in _Fire\n",
            "    component, remaining_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 542, in _CallCallable\n",
            "    result = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1592, in __exit__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 728, in close\n",
            "    tf_session.TF_CloseSession(self._session)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}